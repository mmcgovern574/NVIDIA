{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmcgovern574/NVIDIA/blob/main/notebooks/vectorstore_RAG_chain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e",
      "metadata": {
        "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e"
      },
      "source": [
        "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Qk4Uw_iSr3Mc",
      "metadata": {
        "id": "Qk4Uw_iSr3Mc"
      },
      "source": [
        "<br>\n",
        "\n",
        "# <font color=\"#76b900\">**Notebook 7:** Retrieval-Augmented Generation with Vector Stores</font>\n",
        "\n",
        "<br>\n",
        "\n",
        "In the previous notebook, we learned about embedding models and exercised some of their capabilities. We discussed their intended use cases of longer-form document comparison and found ways to use it as a backbone for more custom semantic comparisons. This notebook will progress these ideas toward the retrieval model's intended use case and explore how to build chatbot systems that rely on *vector stores* to automatically save and retrieve information.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Learning Objectives:**\n",
        "\n",
        "- Understand how semantic-similarity-backed systems can facilitate easy-to-use retrieval formulations.\n",
        "\n",
        "- Learn how to incorporate retrieval modules into your chat model systems for a retrieval-augmented generation (RAG) pipeline, which can be applied to tasks like document retrieval and conversation memory buffers.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Questions To Think About:**\n",
        "\n",
        "- This notebook does not attempt to incorporate hierarchical reasoning or non-naive RAG (such as planning agents). Consider what modifications would be necessary to make these components work in an LCEL chain.\n",
        "\n",
        "- Consider when it would be best to move your vector store solution into a scalable service and when a GPU will become necessary for optimization.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Notebook Source:**\n",
        "\n",
        "- This notebook is part of a larger [**NVIDIA Deep Learning Institute**](https://www.nvidia.com/en-us/training/) course titled [**Building RAG Agents with LLMs**](https://learn.next.courses.nvidia.com/courses/course-v1:DLI+S-FX-15+V1/about). If sharing this material, please give credit and link back to the original course.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "### **Environment Setup:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5XmeiiOWtuxC",
      "metadata": {
        "id": "5XmeiiOWtuxC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "660691cb-318d-4369-ef5b-04faa579a2e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m287.5/287.5 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m104.2/104.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "#%%capture\n",
        "## ^^ Comment out if you want to see the pip install process\n",
        "\n",
        "## Necessary for Colab, not necessary for course environment\n",
        "%pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
        "%pip install -q arxiv pymupdf faiss-cpu\n",
        "\n",
        "## If you're in colab and encounter a typing-extensions issue,\n",
        "##  restart your runtime and try again\n",
        "from langchain_nvidia_ai_endpoints._common import NVEModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BUCKKgF-tySx",
      "metadata": {
        "id": "BUCKKgF-tySx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48056e1a-42b3-4e32-e669-d6156ecc2fcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA API Key: 路路路路路路路路路路\n",
            "Retrieved NVIDIA_API_KEY beginning with \"nvapi-unp...\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'playground_nvolveqa_40k': '091a03bb-7364-4087-8090-bd71e9277520',\n",
              " 'playground_smaug_72b': '008cff6d-4f4c-4514-b61e-bcfad6ba52a7',\n",
              " 'playground_nemotron_qa_8b': '0c60f14d-46cb-465e-b994-227e1c3d5047',\n",
              " 'playground_mistral_7b': '35ec3354-2681-4d0e-a8dd-80325dcf7c63',\n",
              " 'ai-gemma-2b': '04174188-f742-4069-9e72-d77c2b77d3cb',\n",
              " 'ai-parakeet-ctc-riva': '22164014-a6cc-4a6f-b048-f3a303e745bb',\n",
              " 'playground_llama2_70b': '0e349b44-440a-44e1-93e9-abe8dcb27158',\n",
              " 'playground_nemotron_steerlm_8b': '1423ff2f-d1c7-4061-82a7-9e8c67afd43a',\n",
              " 'ai-rerank-qa-mistral-4b': '0bf77f50-5c35-4488-8e7a-f49bb1974af6',\n",
              " 'ai-recurrentgemma-2b': '2f495340-a99f-4b4b-89bd-1beb003dd896',\n",
              " 'playground_deplot': '3bc390c7-eeec-40f7-a64d-0c6a719985f7',\n",
              " 'ai-molmim-generate': '72be0b68-179f-412c-ac03-9a481f78cb9f',\n",
              " 'playground_kosmos_2': '0bcd1a8c-451f-4b12-b7f0-64b4781190d1',\n",
              " 'ai-llama2-70b': '2fddadfb-7e76-4c8a-9b82-f7d3fab94471',\n",
              " 'playground_yi_34b': '347fa3f3-d675-432c-b844-669ef8ee53df',\n",
              " 'playground_starcoder2_15b': '6acada03-fe2f-4e4d-9e0a-e711b9fd1b59',\n",
              " 'playground_gemma_2b': '5bde8f6f-7e83-4413-a0f2-7b97be33988e',\n",
              " 'ai-microsoft-kosmos-2': '6018fed7-f227-48dc-99bc-3fd4264d5037',\n",
              " 'playground_phi2': '6251d6d2-54ee-4486-90f4-2792bf0d3acd',\n",
              " 'playground_gemma_7b': '1361fa56-61d7-4a12-af32-69a3825746fa',\n",
              " 'playground_sdxl': '89848fb8-549f-41bb-88cb-95d6597044a4',\n",
              " 'playground_clip': '8c21289c-0b18-446d-8838-011b7249c513',\n",
              " 'playground_cuopt': '8f2fbd00-2633-41ce-ab4e-e5736d74bff7',\n",
              " 'ai-vista-3d': '72311276-923f-4478-a506-d5b80914728a',\n",
              " 'playground_neva_22b': '8bf70738-59b9-4e5f-bc87-7ab4203be7a0',\n",
              " 'ai-example': '80a5d6c6-7658-49c5-b2b0-105bfb210282',\n",
              " 'playground_seamless': '72ad9555-2e3d-4e73-9050-a37129064743',\n",
              " 'playground_nv_llama2_rlhf_70b': '7b3e3361-4266-41c8-b312-f5e33c81fc92',\n",
              " 'playground_mixtral_8x7b': '8f4118ba-60a8-4e6b-8574-e38a4067a4a3',\n",
              " 'ai-google-deplot': '784a8ca4-ea7d-4c93-bb46-ec027c3fae47',\n",
              " 'ai-ai-weather-forecasting': '9cec444c-db1c-4525-9c6f-f40e4a5b11ce',\n",
              " 'ai-stable-video-diffusion': '8cd594f1-6a4d-4f8f-82b4-d1bf89adae98',\n",
              " 'ai-mixtral-8x7b-instruct': 'a1e53ece-bff4-44d1-8b13-c009e5bf47f6',\n",
              " 'ai-stable-diffusion-xl': 'c1b63bb0-448b-4e53-b2a7-fb0b3723cbe2',\n",
              " 'playground_llama2_code_70b': '2ae529dc-f728-4a46-9b8d-2697213666d8',\n",
              " 'playground_fuyu_8b': '9f757064-657f-4c85-abd7-37a7a9b6ee11',\n",
              " 'playground_llama_guard': 'b34280ac-24e4-4081-bfaa-501e9ee16b6f',\n",
              " 'ai-codellama-70b': 'f6b06895-d073-4714-8bb2-26c09e9f6597',\n",
              " 'playground_llama2_13b': 'e0bb7fb9-5333-4a27-8534-c6288f921d3f',\n",
              " 'ai-gemma-7b': 'a13e3bed-ca42-48f8-b3f1-fbc47b9675f9',\n",
              " 'ai-nvidia-cuopt': 'b0ac1378-3d00-43cb-a8d9-0f0c37ef36c0',\n",
              " 'playground_llama2_code_13b': 'f6a96af4-8bf9-4294-96d6-d71aa787612e',\n",
              " 'ai-fuyu-8b': 'e598bfc1-b058-41af-869d-556d3c7e1b48',\n",
              " 'ai-mistral-7b-instruct-v2': 'd7618e99-db93-4465-af4d-330213a7f51f',\n",
              " 'ai-esmfold': 'a68c59e0-47a6-4a50-bf64-6d88766d56bf',\n",
              " 'ai-embed-qa-4': '09c64e32-2b65-4892-a285-2f585408d118',\n",
              " 'playground_mamba_chat': '381be320-4721-4664-bd75-58f8783b43c7',\n",
              " 'ai-diffdock': 'f3dda972-561a-4772-8c09-873594b6fb72',\n",
              " 'ai-codegemma-7b': '7dfc10a8-3cc4-448e-97c1-2213308dc222',\n",
              " 'playground_steerlm_llama_70b': 'd6fe6881-973a-4279-a0f8-e1d486c9618d',\n",
              " 'playground_llama2_code_34b': 'df2bee43-fb69-42b9-9ee5-f4eabbeaf3a8',\n",
              " 'ai-neva-22b': 'bc205f8e-1740-40df-8d32-c4321763498a',\n",
              " 'ai-sdxl-turbo': 'f886140c-424e-4c82-a841-99e23f9ae35d'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from functools import partial\n",
        "from rich.console import Console\n",
        "from rich.style import Style\n",
        "from rich.theme import Theme\n",
        "\n",
        "console = Console()\n",
        "base_style = Style(color=\"#76B900\", bold=True)\n",
        "pprint = partial(console.print, style=base_style)\n",
        "\n",
        "from getpass import getpass\n",
        "import requests\n",
        "import os\n",
        "\n",
        "hard_reset = False  ## <-- Set to True if you want to reset your NVIDIA_API_KEY\n",
        "while \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\") or hard_reset:\n",
        "    try:\n",
        "        assert not hard_reset\n",
        "        response = requests.get(\"http://docker_router:8070/get_key\").json()\n",
        "        assert response.get('nvapi_key')\n",
        "    except: response = {'nvapi_key' : getpass(\"NVIDIA API Key: \")}\n",
        "    os.environ[\"NVIDIA_API_KEY\"] = response.get(\"nvapi_key\")\n",
        "    try: requests.post(\"http://docker_router:8070/set_key/\", json={'nvapi_key' : os.environ[\"NVIDIA_API_KEY\"]}).json()\n",
        "    except: pass\n",
        "    hard_reset = False\n",
        "    if \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\"):\n",
        "        print(\"[!] API key assignment failed. Make sure it starts with `nvapi-` as generated from the model pages.\")\n",
        "\n",
        "print(f\"Retrieved NVIDIA_API_KEY beginning with \\\"{os.environ.get('NVIDIA_API_KEY')[:9]}...\\\"\")\n",
        "from langchain_nvidia_ai_endpoints._common import NVEModel\n",
        "NVEModel().available_models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3ced9b0-30ed-4ccc-936f-ca03d6e172bf",
      "metadata": {
        "id": "a3ced9b0-30ed-4ccc-936f-ca03d6e172bf"
      },
      "source": [
        "----\n",
        "\n",
        "<br>\n",
        "\n",
        "## Part 1: Summary of RAG Workflows\n",
        "\n",
        "This notebook will explore several paradigms and derive reference code to help you approach some of the most common retrieval-augmented workflows. Specifically, the following sections will be covered (with the differences highlighted):\n",
        "\n",
        "<br>\n",
        "\n",
        "> ***Vector Store Workflow for Conversational Exchanges:***\n",
        "- Generate semantic embedding for each new conversation.\n",
        "- Add the message body to a vector store for retrieval.\n",
        "- Query the vector store for relevant messages to fill in the LLM context.\n",
        "\n",
        "<br>\n",
        "\n",
        "> ***Modified Workflow for an Arbitrary Document:***\n",
        "- **Divide the document into chunks and process them into useful messages.**\n",
        "- Generate semantic embedding for each **new document chunk**.\n",
        "- Add the **chunk bodies** to a vector store for retrieval.\n",
        "- Query the vector store for relevant **chunks** to fill in the LLM context.\n",
        "    - ***Optional:* Modify/synthesize results for better LLM results.**\n",
        "\n",
        "<br>\n",
        "\n",
        "> **Extended Workflow for a Directory of Arbitrary Documents:**\n",
        "- Divide **each document** into chunks and process them into useful messages.\n",
        "- Generate semantic embedding for each new document chunk.\n",
        "- Add the chunk bodies to **a scalable vector database for fast retrieval**.\n",
        "    - ***Optional*: Exploit hierarchical or metadata structures for larger systems.**\n",
        "- Query the **vector database** for relevant chunks to fill in the LLM context.\n",
        "    - *Optional:* Modify/synthesize results for better LLM results.\n",
        "\n",
        "<br>\n",
        "\n",
        "Some of the most important terminology surrounding RAG is covered in detail on the [**LlamaIndex Concepts page**](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html), which itself is a great starting point for progressing towards the LlamaIndex loading and retrieving strategy. We highly recommend using it as a reference as you continue with this notebook and advise you to try out LlamaIndex after the course to consider the pros and cons firsthand!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baa1911b-a6a2-47c5-bc66-1b61e6516437",
      "metadata": {
        "id": "baa1911b-a6a2-47c5-bc66-1b61e6516437"
      },
      "source": [
        "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1cFbKbVvLLnFPs3yWCKIuzXkhBWh6nLQY\" width=1200px/> -->\n",
        "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/data_connection_langchain.jpeg\" width=1200px/>\n",
        ">\n",
        "> From [**Retrieval | LangChain**锔](https://python.langchain.com/docs/modules/data_connection/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XaZ20XoeSTD-",
      "metadata": {
        "id": "XaZ20XoeSTD-"
      },
      "source": [
        "----\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Part 2:** RAG for Conversation History\n",
        "\n",
        "In our previous explorations, we delved into the capabilities of document embedding models and used them to embed, store, and compare semantic vector representations of text. Though we could motivate how to efficiently extend this into vector store land manually, the true beauty of working with a standard API is its strong incorporation with other frameworks that can already do the heavy lifting for us!\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LRx0XUf_Sdxw",
      "metadata": {
        "id": "LRx0XUf_Sdxw"
      },
      "source": [
        "### **Step 1**: Getting A Conversation\n",
        "\n",
        "Consider a conversation crafted using Llama-13B between a chat agent and a blue bear named Beras. This dialogue, dense with details and potential diversions, provides a rich dataset for our study:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IUfCuMkoShWI",
      "metadata": {
        "id": "IUfCuMkoShWI"
      },
      "outputs": [],
      "source": [
        "conversation = [  ## This conversation was generated partially by an AI system, and modified to exhibit desirable properties\n",
        "    \"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the rocky mountains?\",\n",
        "    \"[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch across North America\",\n",
        "    \"[Beras] Wow, that sounds amazing! Ive never been to the Rocky Mountains before, but Ive heard many great things about them.\",\n",
        "    \"[Agent] I hope you get to visit them someday, Beras! It would be a great adventure for you!\"\n",
        "    \"[Beras] Thank you for the suggestion! Ill definitely keep it in mind for the future.\",\n",
        "    \"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research online or watching documentaries about them.\"\n",
        "    \"[Beras] I live in the arctic, so I'm not used to the warm climate there. I was just curious, ya know!\",\n",
        "    \"[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains and their significance!\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tDL2tAo2Skh2",
      "metadata": {
        "id": "tDL2tAo2Skh2"
      },
      "source": [
        "Using the manual embedding strategy from the previous notebook is still very viable, but we can also rest easy and let a **vector store** do all that work for us!\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5hIp943mSqGZ",
      "metadata": {
        "id": "5hIp943mSqGZ"
      },
      "source": [
        "### **Step 2:** Constructing Our Vector Store Retriever\n",
        "\n",
        "To streamline similarity queries on our conversation, we can employ a vector store to help keep track of passages for us! **Vector Stores**, or vector storage systems, abstract away most of the low-level details of the embedding/comparison strategies and provide a simple interface to load and compare vectors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pnaOBgexS-kp",
      "metadata": {
        "id": "pnaOBgexS-kp"
      },
      "source": [
        "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1ZjwYbSZzsXK6ZP8O1-cY3BeRffV4oqzb\" width=1000px/> -->\n",
        "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/vector_stores.jpeg\" width=1200px/>\n",
        ">\n",
        "> From [**Vector Stores | LangChain**锔](https://python.langchain.com/docs/modules/data_connection/vectorstores/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DwZUh6kgS5Ki",
      "metadata": {
        "id": "DwZUh6kgS5Ki"
      },
      "source": [
        "<br>\n",
        "\n",
        "In addition to simplifying the process from an API perspective, vector stores also implement connectors, integrations, and optimizations under the hood. In our case, we will start with the [**FAISS vector store**](https://python.langchain.com/docs/integrations/vectorstores/faiss), which integrates a LangChain-compatable Embedding model with the [**FAISS (Facebook AI Similarity Search)**](https://github.com/facebookresearch/faiss) library to make the process fast and scalable on our local machine!\n",
        "\n",
        "**Specifically:**\n",
        "\n",
        "1. We can feed our conversation into [**a FAISS vector store**](https://python.langchain.com/docs/integrations/vectorstores/faiss) via the `from_texts` constructor. This will take our conversational data and the embedding model to create a searchable index over our discussion.\n",
        "2. This vector store can then be \"interpreted\" as a retriever, supporting the LangChain runnable API and returning documents retrieved via an input query.\n",
        "\n",
        "The following shows how you can construct a FAISS vector store and reinterpret it as a retriever using the LangChain `vectorstore` API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1kE2-ejoTKKU",
      "metadata": {
        "id": "1kE2-ejoTKKU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7a9d225-b8dc-4a03-8304-0293db3dfa23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 185 ms, sys: 12.2 ms, total: 197 ms\n",
            "Wall time: 1.23 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "## ^^ This cell will be timed to see how long the conversation embedding takes\n",
        "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "embedder = NVIDIAEmbeddings(model=\"nvolveqa_40k\")\n",
        "\n",
        "## Streamlined from_texts FAISS vectorstore construction from text list\n",
        "convstore = FAISS.from_texts(conversation, embedding=embedder)\n",
        "retriever = convstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "muN66v5PW5dW",
      "metadata": {
        "id": "muN66v5PW5dW"
      },
      "source": [
        "The retriever can now be used like any other LangChain runnable to query the vector store for some relevant documents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kNZJTnlEWVYh",
      "metadata": {
        "id": "kNZJTnlEWVYh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "outputId": "1b7aa5ef-c839-499b-f6e0-d03f41bc8ca7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m[\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mUser\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the \u001b[0m\n",
              "\u001b[32mrocky mountains?\"\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Absolutely! Lets continue the conversation and explore more about the Rocky Mountains\u001b[0m\n",
              "\u001b[32mand their significance!'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m The Rocky Mountains are a beautiful and majestic range of mountains that stretch \u001b[0m\n",
              "\u001b[32macross North America'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m In the meantime, you can learn more about the Rocky Mountains by doing some research \u001b[0m\n",
              "\u001b[32monline or watching documentaries about them.\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I live in the arctic, so I'm not used to the warm climate \u001b[0m\n",
              "\u001b[32mthere. I was just curious, ya know!\"\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">rocky mountains?\"</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and their significance!'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">across North America'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">online or watching documentaries about them.[Beras] I live in the arctic, so I'm not used to the warm climate </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">there. I was just curious, ya know!\"</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    )</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "pprint(retriever.invoke(\"What is your name?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SE1eDZTEWScC",
      "metadata": {
        "id": "SE1eDZTEWScC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "outputId": "105da88b-0264-4bc5-f117-d9da43ada090"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m[\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m The Rocky Mountains are a beautiful and majestic range of mountains that stretch \u001b[0m\n",
              "\u001b[32macross North America'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m In the meantime, you can learn more about the Rocky Mountains by doing some research \u001b[0m\n",
              "\u001b[32monline or watching documentaries about them.\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I live in the arctic, so I'm not used to the warm climate \u001b[0m\n",
              "\u001b[32mthere. I was just curious, ya know!\"\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Absolutely! Lets continue the conversation and explore more about the Rocky Mountains\u001b[0m\n",
              "\u001b[32mand their significance!'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mUser\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the \u001b[0m\n",
              "\u001b[32mrocky mountains?\"\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">across North America'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">online or watching documentaries about them.[Beras] I live in the arctic, so I'm not used to the warm climate </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">there. I was just curious, ya know!\"</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and their significance!'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">rocky mountains?\"</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    )</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "pprint(retriever.invoke(\"Where are the Rocky Mountains?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mtNCEXLYTVf4",
      "metadata": {
        "id": "mtNCEXLYTVf4"
      },
      "source": [
        "As we can see, our retriever found a handful of semantically relevant documents from our query. You may notice that not all of the documents are useful or clear on their own. For example, a retrieval of *\"Beras\"* for *\"your name\"* may be problematic for the chatbot if provided out of context. Anticipating the potential problems and creating synergies between your LLM components can increase the likelihood of good RAG behavior, so keep an eye out for such pitfalls and opportunities.\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZEDEzpqmTYMv",
      "metadata": {
        "id": "ZEDEzpqmTYMv"
      },
      "source": [
        "### **Step 3:** Incorporating Conversation Retrieval Into Our Chain\n",
        "\n",
        "Now that we have our loaded retriever component as a chain, we can incorporate it into our existing chat system as before. Specifically, we can start with an ***always-on RAG formulation*** where:\n",
        "- **A retriever is always retrieving context by default**.\n",
        "- **A generator is acting on the retrieved context**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uue5UY3_TcvF",
      "metadata": {
        "id": "uue5UY3_TcvF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6b2a025f-3e81-416e-e2f0-1308306db1ff"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mThe context does not provide enough information to provide a specific answer to the question. Based on the \u001b[0m\n",
              "\u001b[1;38;2;118;185;0minformation given, it can be concluded that Beras lives somewhere, but the exact location is not mentioned.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The context does not provide enough information to provide a specific answer to the question. Based on the </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">information given, it can be concluded that Beras lives somewhere, but the exact location is not mentioned.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from langchain.document_transformers import LongContextReorder\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableLambda\n",
        "from langchain.schema.runnable.passthrough import RunnableAssign\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "\n",
        "from functools import partial\n",
        "from operator import itemgetter\n",
        "\n",
        "########################################################################\n",
        "## Utility Runnables/Methods\n",
        "def RPrint(preface=\"\"):\n",
        "    \"\"\"Simple passthrough \"prints, then returns\" chain\"\"\"\n",
        "    def print_and_return(x, preface):\n",
        "        print(f\"{preface}{x}\")\n",
        "        return x\n",
        "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
        "\n",
        "def docs2str(docs, title=\"Document\"):\n",
        "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
        "    out_str = \"\"\n",
        "    for doc in docs:\n",
        "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
        "        if doc_name:\n",
        "            out_str += f\"[Quote from {doc_name}] \"\n",
        "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
        "    return out_str\n",
        "\n",
        "## Optional; Reorders longer documents to center of output text\n",
        "long_reorder = RunnableLambda(LongContextReorder().transform_documents)\n",
        "########################################################################\n",
        "\n",
        "llm = ChatNVIDIA(model=\"mixtral_8x7b\") | StrOutputParser()\n",
        "\n",
        "context_prompt = ChatPromptTemplate.from_messages([\n",
        "    ('system',\n",
        "        \"Answer the question using only the context\"\n",
        "        \"\\n\\nQuestion: {question}\\n\\nContext: {context}\" ## Double reinforcement\n",
        "    ), ('user', \"{question}\"),\n",
        "])\n",
        "\n",
        "chain = (\n",
        "    {\n",
        "        'context': convstore.as_retriever() | long_reorder | docs2str,\n",
        "        'question': (lambda x:x)\n",
        "    }\n",
        "    | context_prompt\n",
        "    # | RPrint()\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "pprint(chain.invoke(\"Where does Beras live?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FSIqTMuuTjIh",
      "metadata": {
        "id": "FSIqTMuuTjIh"
      },
      "source": [
        "Take a second to try out some more invocations and see how the new setup performs. Regardless of your model choice, the following questions should serve as interesting starting points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4jDJwrYpTmpd",
      "metadata": {
        "id": "4jDJwrYpTmpd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "c9b7bcaf-208a-4f98-e2f0-44423ba24553"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mThe Rocky Mountains stretch across North America. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mThe exact locations and extents of the Rocky Mountains are not \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mprovided in the given context.\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The Rocky Mountains stretch across North America. (The exact locations and extents of the Rocky Mountains are not </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">provided in the given context.)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "pprint(chain.invoke(\"Where are the Rocky Mountains?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-artagLfTpBy",
      "metadata": {
        "id": "-artagLfTpBy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "779d3dd1-18a9-4a2c-b7d4-3c9865efa7ea"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mThe Rocky Mountains are a range of mountains that stretch across North America. Based on the context provided, \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mthere is no information given about their exact location relative to California.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The Rocky Mountains are a range of mountains that stretch across North America. Based on the context provided, </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">there is no information given about their exact location relative to California.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "pprint(chain.invoke(\"Where are the Rocky Mountains? Are they close to California?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N-4RYuX6TqHA",
      "metadata": {
        "id": "N-4RYuX6TqHA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "278f3e29-3da1-40c1-d47d-9eba09643b57"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mThe Rocky Mountains are a beautiful and majestic range of mountains that stretch across North America. They are \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mlocated in Western North America and run from the northernmost part of British Columbia, in western Canada, to New \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mMexico in the Southwestern United States. The Rocky Mountains are home to a diverse range of wildlife and plant \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mspecies, and they are a popular destination for outdoor activities such as hiking, skiing, and camping. The \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mauthor's reasoning for the location of the Rocky Mountains is based on their knowledge of the geography of North \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mAmerica and the distribution of mountain ranges on the continent. The author also provides additional information \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mand encourages further research to learn more about the Rocky Mountains and their significance.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The Rocky Mountains are a beautiful and majestic range of mountains that stretch across North America. They are </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">located in Western North America and run from the northernmost part of British Columbia, in western Canada, to New </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Mexico in the Southwestern United States. The Rocky Mountains are home to a diverse range of wildlife and plant </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">species, and they are a popular destination for outdoor activities such as hiking, skiing, and camping. The </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">author's reasoning for the location of the Rocky Mountains is based on their knowledge of the geography of North </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">America and the distribution of mountain ranges on the continent. The author also provides additional information </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and encourages further research to learn more about the Rocky Mountains and their significance.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "pprint(chain.invoke(\n",
        "    \"Where are the Rocky Mountains? Please include\"\n",
        "    \" the author's reasoning, but provide more information!\"\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GDgjdfdpTrV5",
      "metadata": {
        "id": "GDgjdfdpTrV5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "517e2997-a93d-4e36-9930-22c62e2b934e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mThe context does not provide information about the distance between Beras' location in the Arctic and the Rocky \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mMountains.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The context does not provide information about the distance between Beras' location in the Arctic and the Rocky </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Mountains.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "pprint(chain.invoke(\"How far away is Beras from the Rocky Mountains?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8wp9-8CbT0L9",
      "metadata": {
        "id": "8wp9-8CbT0L9"
      },
      "source": [
        "<br>\n",
        "\n",
        "You might notice some decent performance with this always-on retrieval node in the loop since the actual context being fed into the LLM remains relatively small. It's important to experiment with factors like embedding sizes, context limits, and model options to see what kinds of behavior you can expect and which efforts are worth taking to improve performance.\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OnpOybOhUCTf",
      "metadata": {
        "id": "OnpOybOhUCTf"
      },
      "source": [
        "### **Step 4:** Automatic Conversation Storage\n",
        "\n",
        "Now that we see how our vector store memory unit should function, we can perform one last integration to allow our conversation to add new entries to our conversation: a runnable that calls the `add_texts` method for us to update the store state.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FsK6-AtRVdcZ",
      "metadata": {
        "id": "FsK6-AtRVdcZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "49a3406a-5ce9-4045-adec-8d07e1b7f16a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mThat sounds like a great plan, Beras! I'm sure you'll enjoy trying all the different flavors of ice cream there. I \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mknow I would! By the way, have you ever tried making your own ice cream at home? It can be a fun and delicious \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mactivity to do on a warm day in the Arctic.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">That sounds like a great plan, Beras! I'm sure you'll enjoy trying all the different flavors of ice cream there. I </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">know I would! By the way, have you ever tried making your own ice cream at home? It can be a fun and delicious </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">activity to do on a warm day in the Arctic.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mBased on your excitement for ice cream, I'm going to guess that it might be one of your favorite foods! Would I be \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mcorrect in guessing that?\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Based on your excitement for ice cream, I'm going to guess that it might be one of your favorite foods! Would I be </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">correct in guessing that?</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mOh, I see! You caught me there! I must have misunderstood earlier. So, honey is your favorite food, huh? That's \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mreally interesting! Have you ever tried adding honey to your homemade ice cream? It could be a great combination!\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Oh, I see! You caught me there! I must have misunderstood earlier. So, honey is your favorite food, huh? That's </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">really interesting! Have you ever tried adding honey to your homemade ice cream? It could be a great combination!</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mBased on our conversation, I would guess that your favorite food is ice cream! You seemed really excited about \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mgetting some and it's a delicious treat. But, just to confirm, is that your favorite food or is there something \u001b[0m\n",
              "\u001b[1;38;2;118;185;0melse you prefer even more?\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Based on our conversation, I would guess that your favorite food is ice cream! You seemed really excited about </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">getting some and it's a delicious treat. But, just to confirm, is that your favorite food or is there something </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">else you prefer even more?</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from operator import itemgetter\n",
        "\n",
        "########################################################################\n",
        "## Reset knowledge base and define what it means to add more messages.\n",
        "convstore = FAISS.from_texts(conversation, embedding=embedder)\n",
        "\n",
        "def save_memory_and_get_output(d, vstore):\n",
        "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
        "    vstore.add_texts([f\"User said {d.get('input')}\", f\"Agent said {d.get('output')}\"])\n",
        "    return d.get('output')\n",
        "\n",
        "########################################################################\n",
        "\n",
        "llm = ChatNVIDIA(model=\"mixtral_8x7b\") | StrOutputParser()\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
        "    \"A user has asked a question: {input}\\n\\n Context: \\n{context}\\n\\n\"\n",
        "    \"Please continue the conversation by responding! Keep it brief and conversational!\"\n",
        "), ('user', '{input}')])\n",
        "\n",
        "conv_chain = (\n",
        "    {\n",
        "        'context': convstore.as_retriever() | long_reorder | docs2str,\n",
        "        'input': (lambda x:x)\n",
        "    }\n",
        "    | RunnableAssign({'output' : chat_prompt | llm})\n",
        "    | partial(save_memory_and_get_output, vstore=convstore)\n",
        ")\n",
        "\n",
        "pprint(conv_chain.invoke(\"I'm glad you agree! I can't wait to get some ice cream there! It's such a good food!\"))\n",
        "print()\n",
        "pprint(conv_chain.invoke(\"Can you guess what my favorite food is?\"))\n",
        "print()\n",
        "pprint(conv_chain.invoke(\"Actually, it's honey! Not sure where you got that idea?\"))\n",
        "print()\n",
        "pprint(conv_chain.invoke(\"I see! Fair enough! Do you know my favorite food now?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KRMW6G7NVSWF",
      "metadata": {
        "id": "KRMW6G7NVSWF"
      },
      "source": [
        "Unlike the more automatic full-text or rule-based approaches to injecting context into the LLM, this approach ensures some amount of consolidation which can keep the context length from getting out of hand. It's still not a full-proof strategy on its own, but it's a stark improvement for unstructured conversations (and doesn't even require a strong instruction-tuned model to perform slot-filling)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9TPkh3SaLbqh",
      "metadata": {
        "id": "9TPkh3SaLbqh"
      },
      "source": [
        "----\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Part 3 [Exercise]:** RAG For Document Chunk Retrieval\n",
        "\n",
        "Given our prior exploration of document loading, the idea that data chunks can be embedded and searched through probably isn't surprising. With that said, it is definitely worth going over since applying RAG with documents is a double-edged sword; it may **seem** to work well out of the box but requires some extra care when optimizing it for truly reliable performance. It also provides an excellent opportunity to review some fundamental LCEL skills, so let's see what we can do!\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Exercise:**\n",
        "\n",
        "In the previous example, you may recall that we pulled in some relatively small papers with the help of [`ArxivLoader`](https://python.langchain.com/docs/integrations/document_loaders/arxiv) using the following syntax:\n",
        "\n",
        "```python\n",
        "from langchain.document_loaders import ArxivLoader\n",
        "\n",
        "docs = [\n",
        "    ArxivLoader(query=\"2205.00445\").load(),  ## MRKL\n",
        "    ArxivLoader(query=\"2210.03629\").load(),  ## ReAct\n",
        "]\n",
        "```\n",
        "\n",
        "Given all that you've learned so far, choose a selection of papers that you would like to use and develop a chatbot that can talk about them!\n",
        "\n",
        "<br>\n",
        "\n",
        "Though this is a pretty big task, a walkthrough of ***most*** of the process will be provided below. By the end of the walkthrough, many of the necessary puzzle pieces will be provided, and your real task will be to integrate them together for the final `retrieval_chain`. When you're done, get ready to re-integrate the chain (or a flavor of your choice) in the last notebook as part of the evaluation exercise!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jSjfCtiQnj9e",
      "metadata": {
        "id": "jSjfCtiQnj9e"
      },
      "source": [
        "<br>\n",
        "\n",
        "### **Task 1**: Loading And Chunking Your Documents\n",
        "\n",
        "The following code block gives you some default papers to load in for your RAG chain. Feel free to select more papers as desired, but note that longer documents will take longer to process. A few simplifying assumptions and additional processing steps are included to help you improve your naive RAG performance:\n",
        "\n",
        "- Documents are cut off prior to the \"References\" section if one exists. This will keep our system from considering the citations and appendix sections, which tend to be long and distracting.\n",
        "\n",
        "- A chunk that lists the available documents is inserted to provide a high-level view of all available documents in a single chunk. If your pipeline does not provide metadata on each retrieval, this is a useful component and can even be listed among a list of higher-priority pieces if appropriate.\n",
        "\n",
        "- Additionally, the metadata entries are also inserted to provide general information. Ideally, there would also be some synthetic chunks that merge the metadata into interesting cross-document chunks.\n",
        "\n",
        "**NOTE:** ***For the sake of the assessment, please include at least one paper that is less than one month old!***\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDYqcu_szQLs",
        "outputId": "040615d1-f875-440c-d5ac-142fbd138a4e"
      },
      "id": "qDYqcu_szQLs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[?25l     \u001b[90m\u001b[0m \u001b[32m0.0/290.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m204.8/290.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n",
            "Installing collected packages: pypdf\n",
            "Successfully installed pypdf-4.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test syntax\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import ArxivLoader\n",
        "\n",
        "doc1 = ArxivLoader(query=\"2205.00445\").load()\n",
        "doc2 = PyPDFLoader(\"/content/K211951.pdf\").load()\n",
        "\n",
        "print(doc1)\n",
        "print(doc2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rae76OIwPyo6",
        "outputId": "2297624a-21d0-4baf-b359-f88150cd5fee"
      },
      "id": "rae76OIwPyo6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='MRKL Systems\\nA modular, neuro-symbolic architecture that combines large language\\nmodels, external knowledge sources and discrete reasoning\\nEhud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber,\\nNir Ratner, Yoav Shoham, Ho铿t Bata, Yoav Levine, Kevin Leyton-Brown,\\nDor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai\\nShalev-Shwartz, Amnon Shashua, Moshe Tenenholtz\\nAI21 Labs\\nMay 3, 2022\\nAbstract\\nHuge language models (LMs) have ushered in a new era for AI, serving as a gate-\\nway to natural-language-based knowledge tasks. Although an essential element of\\nmodern AI, LMs are also inherently limited in a number of ways. We discuss these\\nlimitations and how they can be avoided by adopting a systems approach. Con-\\nceptualizing the challenge as one that involves knowledge and reasoning in addition\\nto linguistic processing, we de铿ne a 铿exible architecture with multiple neural mod-\\nels, complemented by discrete knowledge and reasoning modules. We describe this\\nneuro-symbolic architecture, dubbed the Modular Reasoning, Knowledge and Lan-\\nguage (MRKL, pronounced miracle) system, some of the technical challenges in\\nimplementing it, and Jurassic-X, AI21 Labs MRKL system implementation.\\n1\\nIntroduction\\nHuge language models (LMs) such as BERT [1], GPT-3 [2], Jurassic-1 [3], PaLM [4],\\nand others [59], have taken AI by storm, with the promise of serving as versatile,\\ngeneral-purpose foundations for many applications. Indeed, partly for this reason,\\nthey have been rebranded by some as foundation models [10]. The term is meant\\nbroadly, covering language models as well as models that were trained on more than\\njust text, and although such multimodal models are not the focus of this paper,\\ntheres another reason to take the term language model with a grain of salt. While\\nLMs indeed model syntax, and other linguistic elements, their most striking feature\\nis that they model the world, as described by the data on which they were trained.\\n1\\narXiv:2205.00445v1  [cs.CL]  1 May 2022\\nAnd so really LMs serve as a textual gateway to the universe of knowledge [11, 12],\\nand perhaps should instead be called language and knowledge models.\\nWhen viewed this way, it becomes clear that, despite their value, current LMs\\nhave inherent limitations. While versatile and impressive, the output of even huge\\nLMs is in many cases wrong, and often ridiculously so [13]. Here is a sample output\\nof GPT-3 on some simple queries. (To be clear, this is not a critique of GPT-3 specif-\\nically, and other LMs  including our own Jurassic-1  exhibit similar silliness.)\\nFor example, LMs can struggle to understand that there are no US cities with\\nmore than 20m citizens, that a math teacher is a person, dont know what todays\\ndate is, nor can they engage in even simple (e.g., mathematical) reasoning.\\nWhen you look for the root cause, you realize the core limitations of LMs: They\\ndont have access to all relevant knowledge, and neural models are ill-suited for\\ncertain types of calculation. More speci铿cally:\\n1. Lack of access to current information. Certain data constantly change  the\\nexchange rate between the dollar and the Moroccan Dirham, current COVID\\nnumbers, the stock price of AAPL, the weather in Vancouver (OK, not so\\nmuch), or even the current date. Its impossible, by their design, for pretrained\\nlanguage models to keep up with this dynamic information [14].\\n2. Lack of access to proprietary information sources.\\nAs an important special\\ncase of 1, the models dont have access to proprietary information, such as\\nthe client roster in a companys database or the state of an online game.\\n3. Lack of reasoning. Certain reasoning is beyond the reach of the neural ap-\\nproach, and requires a dedicated reasoning process. We saw above the classic\\nexample of arithmetic reasoning. GPT-3 and Jurassic-1 perform well on 2-digit\\naddition, which is impressive, but con铿dently spit out nonsensical answers on\\n4-digit additions. With increased training time, better data, and larger mod-\\nels, the performance of LMs will improve, but will not reach the robustness of\\nan HP calculator from the 1970s. And mathematical reasoning is just the tip\\nof an iceberg.\\n2\\nIn addition to these shortcomings, there is another inherent problem with the\\ntraditional approach to deploying LMs:\\n4. Model explosion. Todays LMs zero-shot performance trails that of 铿ne-tuned\\nmodels. One can 铿ne-tune the LM to a speci铿c task, but then lose versatil-\\nity. Contemporary e铿orts to mitigate the problem focus on training a huge\\nLM jointly on many sets of curated NLP tasks in a massive multi-task setting\\n(several leading studies reaching 100+ tasks) [6, 7, 15, 16]. These formidable\\ne铿orts are e铿ective; the resulting models exhibit versatility and high perfor-\\nmance when encountering inputs resembling those of the curated tasks. But\\nthe performance of these models on tasks that are not close enough to those\\nincluded in the curated tasks can signi铿cantly deteriorate (for example, per-\\nplexity degrades signi铿cantly). It is not practical to 铿ne-tune and serve multi-\\nple large models. Nor can one further tune a multi-task-trained LM [6, 7, 15,\\n16] on a new task that hadnt been covered in its training; due to catastrophic\\nforgetting, adding the new task necessitates retraining on the entire task set.\\nGiven the cost of training such models [1719], this is clearly infeasible to do\\nrepeatedly.\\nDespite all these shortcomings, large language models are an essential backbone\\nof any future AI system. So the question is how to have our cake and eat it too,\\nenjoying the bene铿ts of self-supervised deep language models without su铿ering these\\ndrawbacks. The solution we o铿er takes the form of a 铿exible architecture dubbed\\nthe Modular Reasoning, Knowledge and Language (MRKL, pronounced miracle)\\nsystem, whose high-level design is depicted below.\\nA\\n3\\nThus a MRKL system consists of an extendable set of modules, which we term\\nexperts, and a router that routes every incoming natural language input to a module\\nthat can best respond to the input (the output of that module can be the output of\\nthe MRKL system, or be routed to another module). These modules can be:\\n Neural, including the general-purpose huge language model as well as other\\nsmaller, specialized LMs.\\n Symbolic, for example a math calculator, a currency converter or an API call\\nto a database.\\nMRKL systems enjoy important bene铿ts when compared to 铿ne-tuned multi-task\\nmodels:\\n1. Safe fallback: In case the input doesnt match any existing expert module, the\\nrouter sends the input directly to the general-purpose huge LM.\\n2. Robust extensibility: Since each expert is trained independently we are able to\\ncheaply add new capabilities while guaranteeing that they do not compromise\\nthe performance of existing ones. The only component that requires retraining\\nis the router which is a relatively lightweight task.\\n3. Interpretability: When the router invokes a speci铿c module, that often has the\\nside bene铿t of providing a rationale for the MRKL systems output (1+1 = 2\\nbecause the calculator said so); such explanations are crucially lacking in\\nexisting language models.\\n4. Up-to-date information: The integration of external APIs allows the MRKL\\nsystem to hook into dynamic knowledge bases, and correctly answer inputs\\nthat static models cannot.\\n5. Proprietary knowledge: Access to proprietary databases and other information\\nsources.\\n6. Compositionality: By routing compounded multi-hop inputs to di铿erent ex-\\nperts we are able to naturally integrate their responses and correctly address\\ncomplex inputs [20].\\n2\\nJurassic-X: AI21 Labs MRKL system\\nWe have implemented a MRKL system called Jurassic-X, which is being piloted by\\na few partners.\\nJurassic-X will soon be available to developers; you can apply for early access here.\\nMeanwhile, you can experience Jurassic-X via a demo (link).\\n4\\n3\\nCrossing the neuro-symbolic chasm: A calculator test\\ncase\\nThere are of course many details involved in implementing a MRKL system. In\\nconnection with avoiding model explosion, see our detailed discussion here. There\\nis also an interesting challenge of how to intelligently route input among modules,\\nwhich we leave for a separate discussion. Here we discuss one particularly interest-\\ning challenge, that of extracting from the text the formal arguments to symbolic\\nreasoners.\\n3.1\\nArithmetic as a test case for chasm crossing\\nThe relation between neural and symbolic approaches to AI is guaranteed to raise\\nheated discussions about their relative merits with neuro-skeptics on one extreme,\\nneuro-diehards on the other, and the rest trying to have a rational conversation.\\nBefore discussing our technical approach wed like to clarify our guiding ideology. We\\nbelieve that neural LMs are essential; we didnt build Jurassic-1 [3] for no reason, and\\nit serves as a backbone to our applications as well as those of developers using AI21\\nStudio. But as weve made clear, we also believe they have inherent shortcomings.\\nWe dont take a 铿rm position on which types of computation are best relegated to\\nthe neural machinery, and which should be carved out to symbolic methods. That\\nwill surely be an evolving process. What we do 铿rmly believe is that some tasks\\nshould be handled by discrete methods, and we should have a clear methodology for\\nhow to hand o铿the computation from the neural net to the symbolic procedure.\\nOnce the router has made the decision of which module to call upon, it needs to\\npass the right information to it. The router is a specialized neural net and there-\\nfore invoking a neural module is easy since the neural-to-neural interface is natural.\\nHowever, when a neural network needs to access a database, make an API call, or\\ninvoke another symbolic computation, it must extract from the text discrete param-\\neters required by the module. The main message here is that there is no free lunch,\\nbut that lunch is in many cases a铿ordable. The cost is in training the router to\\nextract the arguments reliably, which must be done rigorously. We make the point\\nby discussing how we trained Jurassic-X to extract basic arithmetic operations.\\nThere are many ways to describe in language a situation that calls for perform-\\ning arithmetic. The most straightforward way is to use mathematical notation, say\\n3 1 =?测. Google search handles it well:\\n5\\nNot much harder is to express it in words, as in How much is three minus 1. Google\\nsearch does this well too:\\nBut then it gets tricky. At the lexical level, one may use di铿erent synonyms that\\ncarry the same meaning: (twelve, 12 and a dozen). But beyond simple lexical\\nissues, there are phrases that require world knowledge to understand that there is\\nan arithmetic exercise encoded in the text (I lost one ball, I dropped one ball,\\nOne ball was taken from me) and what the speci铿c exercise is. Here Google search\\nstarts to stumble:\\nThis is where language models come in. Note that we limit their role to extracting\\nthe right arguments for the symbolic calculator, not performing the calculation itself.\\n6\\nBut we should approach even this task with sober expectations. As anyone knows\\nfrom elementary school, its harder to teach children to solve verbal math problems\\nthan to solve explicitly stated math problems. Its harder for adults too, and its\\nno di铿erent for the computer. At some point the machine will stumble (indeed, at\\nsome point humans will disagree about the underlying math problem). But when\\nwe decide to invoke a symbolic method we should expect reasonable robustness.\\nThe solution is to train the router to extract the right input independently for\\neach module, with rigorous evaluation of the performance. By way of example, here\\nis how it was achieved simple, one- and two-operation arithmetics.\\n3.2\\nTraining Jurassic-X to extract the arguments for basic arith-\\nmetic\\nOur goal here is not to show how we can extract the most complex mathematical\\nexpressions from text, but how we can extract simple expressions with high reliabil-\\nity, the sort of reliability one would need in a production-grade system. We found\\nthat while extensive pretraining does allow Jurassic-1 to extract the arithmetic op-\\neration in many cases in a few-shot setting, the performance is far from perfect. See\\nAppendix A for a detailed comparison. Using a data augmentation methodology,\\nbased on generating examples from a structured example space, we show that the\\nstatic pretrained LM can be made to achieve near perfect performance.\\nWe empirically investigate two related research questions:\\n1. What is required in terms of data augmentation to achieve near-perfect per-\\nformance in the case of basic mathematical operations?\\n2. How well can Jurassic-X generalize between math problems that vary in type\\nor complexity?\\nTo answer these questions we evaluate the models performance on cases where\\nthe test data is drawn from the same distribution as the training data, as well as on\\nout-of-distribution test data (allowing us, for example, to train a model on addition\\nproblems and test it on multiplication problems).\\n3.2.1\\nData Augmentation\\nWe use a small set of templates to generate a set of training and test examples. Each\\nexample is characterized by the following dimensions:\\n1. The numbers used in the arithmetic expression, henceforth the operands. The\\noperands may be digits (e.g., 48) or numbers in word form (e.g., forty eight).\\n2. The number of digits in the operands. We explore numbers of 19 digits.\\n7\\n3. The type of operation. We explore addition, subtraction, multiplication and\\ndivision.\\n4. The number of operations in the arithmetic expression. We explore the cases\\nof one and two operations.\\n5. The placement or absence of brackets (in expressions with two operations). In\\nsome expressions the placement of brackets may change the result. We explore\\nall logical possibilities for the case of two operations.\\nTable 10 in the appendix details the templates we use in this set of experiments\\nfor the case of a single operation. In the case of two operations, there are 29 distinct\\ncombinations that have a natural formulation in natural language (see Table 12).\\nThe data can be found here.\\n3.2.2\\nExperimental setup\\nWe conducted our experiments with the 7B parameters J1-large model [3] using\\nprompt-tuning [21] with 10 prompt tokens. Across all experiments, we set our learn-\\ning rate to be lr = 0.3 and used linear decay. The batch size was set to 32 and we\\nde铿ned a short warm-up depending on the number of steps in each experiment.\\nExperiments 1 and 2 below were trained for 3000 steps, and the reported results\\nhere were the test accuracy evaluated on the 铿nal model. For the remaining exper-\\niments we used linear weight decay (0.001), which we found to be crucial for the\\nmodels performance, and selected the best checkpoint using a validation set. Each\\nexperiment was run 3-5 times, and results show mean 卤 standard deviation across\\nthese runs.\\nIn all experiments we veri铿ed that there was no overlap between the problems\\nincluded in the training set and those included in the test set. This also includes\\navoiding cases with the same underlying arithmetic expression, but using di铿erent\\nwordings for training and testing. For a detailed description of the sizes of the data\\nsplits see Section C.\\n3.2.3\\nResults\\nIn the following results we report the accuracy we achieve in di铿erent experiments,\\nby which we mean the percent of problems in the relevant test set on which the\\nsystem gave the correct answer. (We note again that, since the actual calculation is\\ndone by the calculator module, all errors are due to passing the wrong operations or\\noperands to the calculator.)\\nExperiment 1:\\nGeneralization across di铿erent number of digits in the\\noperands.\\nInspired by the experiments reported by [2], we test Jurassic-Xs ability\\nto generalize to numbers it has not seen in the training data, including numbers\\n8\\nwith a di铿erent (and much larger) number of digits. At training time we expose\\nthe model only to single-digit numbers, while at test time we evaluate on numbers\\nwith 1 to 9 digits. In this section we experiment with simple problems, involving\\nonly numbers written as digits (and not numbers as words), one format (format 0\\nfrom Table 10), and a single operation. We explore two settings: one where that\\noperation is addition, and one where it is multiplication.\\nTable 1 presents our results, sliced by the number of digits, and compared to the\\nresults by GPT-3s approach [2] (as representative of all language models that dont\\nhave access to external calculator), reported on addition. We note that we trained\\non single-digit operations for all settings while GPT-3 was conditioned on examples\\nwith the same number of digits when answering a certain problem. Our results show\\nthat despite the fact that training was only done on numbers with a single digit,\\nthe model is able to generalize to all numbers of digits explored. This is in stark\\ncontrast to the approach of language models which attempt to synthesize arithmetic\\ncapabilities from the training data, and as a result display a dramatic decrease in\\nperformance as the number of digits increases.\\nNum. digits\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nAddition\\n1.0\\n1.0\\n1.0\\n1.0\\n1.0\\n1.0\\n1.0\\n1.0\\n1.0\\nMultiplication\\n1.0\\n1.0\\n1.0\\n1.0\\n1.0\\n0.98\\n1.0\\n1.0\\n1.0\\nGPT-3\\nN/A\\n1.0\\n0.804\\n0.255\\n0.093\\nN/A\\nN/A\\nN/A\\nN/A\\nTable 1:\\nRobustness to the number of digits in the operands. Accuracy vs. the\\nnumber of digits in the test data for a model that was trained only on 1-digit operands\\n(5 + 3, etc.). Results for GPT-3 were taken from [2].\\nExperiment 2: Generalization from digits to numbers as words and vice\\nversa.\\nIn this set of experiments, we test the models ability to generalize from\\narithmetic questions with digits to ones where the numbers are expressed with words,\\nand vice versa. We note that this task is not trivial considering the fact that the\\ntokenization of a number represented with digits is di铿erent than that when the same\\nnumber is represented as words. For example, we train the model with examples\\nsuch as How much is 58 plus 12 and evaluate it on examples such as How much\\nis twenty seven plus thirteen, and vice versa. While we vary the lexical choice for\\nspecifying numbers using words or digits, we hold other dimensions of the arithmetic\\nworld 铿xed, including using only format 0 and holding the type of operation 铿xed\\nat training and test time.\\n(In the following paragraphs we examine these other\\ndimensions.)\\nTable 2 shows the accuracy for all combinations of training and testing on digits\\nand on words. We notice that in-distribution performance (training and testing on\\nthe same type of input) is close to 100 % accuracy. We further observe that training\\n9\\non words generalizes well to digits. Interestingly, the inverse is not true and the per-\\nformance is much lower, indicating the underlying di铿erence in the representations\\nof the numbers.\\nTest\\nTrain\\nDigits\\nWords\\nDigits\\n1.0\\n0.156\\nWords\\n0.987\\n0.988\\nTable 2:\\nRobustness to wordings. Accuracy for all combination of training and testing\\non numbers written as digits and as words. Results are averaged across all the numbers of\\ndigits.\\nExperiment 3: Generalization across question formats.\\nA main challenge\\nin constructing natural language interfaces to discrete reasoners like a calculator is in\\nhandling language variability. One of the most appealing characteristics of language\\nmodels is their ability to abstract away from such variability. Here we test the ability\\nof the model to generalize over formats of arithmetic problems. Experiments are for\\nsingle-operation problems.\\nWe train on a single format (format 0 in Table 10), test on all formats, and\\nbreak the results by format. We report results with all four operation types. Table\\n3 shows that the generalization across formats is perfect in most cases. Format 4,\\nwhich is the only one not phrased as a question (see Table 10) appears to be the most\\nchallenging to generalize to. We note that there are numerous ways to phrase such\\nquestions, hence the ability to generalize across formats, even for the case where the\\nmodel was trained only on one format, is critical for the models success.\\nadd\\nsub\\nmul\\ndiv\\nformat 0\\n1.0 卤 0\\n1.0 卤 0\\n1.0 卤 0\\n0.997 卤 0.006\\nformat 1\\n1.0 卤 0\\n1.0 卤 0\\n1.0 卤 0\\n1.0 卤 0\\nformat 2\\n1.0 卤 0\\n1.0 卤 0\\n1.0 卤 0\\n1.0 卤 0\\nformat 3\\n1.0 卤 0\\n0.993 卤 0.012\\n0.997 卤 0.006\\n1.0 卤 0\\nformat 4\\n0.993 卤 0.012\\n0.863 卤 0.237\\n0.977 卤 0.04\\n0.727 卤 0.465\\nTable 3: Robustness to phrasing. Accuracy when training with one format (format 0)\\nand evaluating on each of the 5 formats from Table 10.\\nExperiment 4: Generalization between operations.\\nNext we explore whether\\na model that was trained on one type of arithmetic problems can generalize to\\nother types.\\nWe conduct two types of experiments: one where we examine the\\n10\\ngeneralization ability of Jurassic-X on single operation problems, and one with two-\\noperation problems.\\nFor the 铿rst set of experiments, we train with one operation at a time, using\\nall formats with numbers as digits. Operands have between 1 and 9 digits. We\\ntest on all types of single-operation problems. The results are shown in in Table 4.\\nConsistent with the previous experiments, training and evaluating on arithmetic\\nproblems with the same operations is consistently successful (accuracy > 0.99 along\\nthe diagonal). Interestingly, we 铿nd strong generalization in many cross-operation\\ncases (o铿the main diagonal in the table). For example, training on addition works\\nalmost perfectly when evaluated on subtraction and multiplication. The division\\noperation is an exception, as models trained on it struggle with multiplication and\\nsubtraction (but perform reasonably on addition). Conversely, models trained on\\nother operations do not generalize very well to division.\\nTest\\nTraining\\nadd\\nsub\\nmul\\ndiv\\nadd\\n0.997 卤 0.006\\n0.96 卤 0.026\\n0.97 卤 0.006\\n0.477 卤 0.127\\nsub\\n0.987 卤 0.023\\n0.997 卤 0.006\\n0.763 卤 0.182\\n0.183 卤 0.196\\nmul\\n0.483 卤 0.466\\n0.817 卤 0.186\\n0.993 卤 0.006\\n0.26 卤 0.2\\ndiv\\n0.787 卤 0.244\\n0.31 卤 0.334\\n0.23 卤 0.214\\n0.993 卤 0.012\\nTable 4: Generalization across operations for single-operation problems. Accuracy\\nwhen training with one operation and evaluating on other operations.\\nFor the second set of experiments, we randomly partition the 29 possible two-\\noperation arithmetic problems (Tables 11, 12) into subsets of 14 and 15 to be used for\\ntraining and testing, respectively. We repeated this process for 10 random splits of\\nthe operations, assuring that only one of the training two-operation problems would\\ninclude brackets. Other settings are as with the 铿rst set of experiments.\\nTable 5 shows the average accuracy for each of the 29 types of two-operation\\nproblems. The results show that in 22 of the 29 combinations the accuracy exceeds\\n90%, indicating reasonable generalization capabilities for such a setting.\\nExperiment 5: Generalization across a di铿erent number of operations.\\nThis 铿nal set of experiments explores the ability of the model to generalize from\\nsingle-operation problems to two-operation problems.\\nAs above, we use all formats (0-4), with operands of 1 to 9 digits (numbers\\nwritten with digits), and train on examples from all four operation types (single-\\noperation problems only). We evaluate on examples with two operations, focusing\\non the situations that do not require bracketing  16 unique combinations in total.\\nThe results are shown in Table 6 and are organized by the 铿rst operation (rows) and\\nsecond operation (columns) in the test problems.\\n11\\nFormula\\nMean\\nSTD\\nf=((A+B)*C)\\n0.288\\n0.267\\nf=(A+B*C)\\n0.406\\n0.349\\nf=((A-B)*C)\\n0.562\\n0.194\\nf=(A/(B/C))\\n0.677\\n0.228\\nf=(A-B*C)\\n0.697\\n0.266\\nf=(A*(B-C))\\n0.797\\n0.325\\nf=((A+B)/C)\\n0.867\\n0.265\\nf=(A-(B-C))\\n0.930\\n0.057\\nf=((A-B)/C)\\n0.940\\n0.120\\nf=(A-B/C)\\n0.955\\n0.042\\nf=(A/(B+C))\\n0.960\\n0.042\\nf=(A/(B-C))\\n0.962\\n0.094\\nf=(A+B/C)\\n0.964\\n0.049\\nf=(A*(B/C))\\n0.970\\n0.052\\nf=(A*B+C)\\n0.973\\n0.025\\nf=(A*(B+C))\\n0.974\\n0.025\\nf=(A/B+C)\\n0.981\\n0.024\\nf=(A/B/C)\\n0.985\\n0.017\\nf=(A/B-C)\\n0.987\\n0.022\\nf=(A/B*C)\\n0.99\\n0.015\\nf=(A-(B+C))\\n0.99\\n0.017\\nf=(A*B-C)\\n0.992\\n0.015\\nf=(A/(B*C))\\n0.995\\n0.012\\nf=(A-B+C)\\n1\\n0\\nf=(A+B+C)\\n1\\n0\\nf=(A-B-C)\\n1\\n0\\nf=(A*B/C)\\n1\\n0\\nf=(A+B-C)\\n1\\n0\\nf=(A*B*C)\\n1\\n0\\nTable 5:\\nGeneralization for two-operation problems. Mean and standard deviation\\nacross 10 partitions of the formulae to train and test.\\nIn almost all cases, we observe excellent performance on solving two-operation\\nproblems, despite seeing only single-operation problems at training time. This re-\\nsult is also important for the systematic development of the capability since the\\nnumber of possible formulae grows rapidly with the number of operations, rendering\\nan exhaustive training of all combinations extremely challenging for multi-operation\\nformulae. There are three exceptions of failure to generalize to two-operation prob-\\nlems: addmul, submul, and subdiv. In these three cases, the templates are of\\n12\\nthe form of two pre铿x phrasings (e.g., what is the sum of 2 and the product of 4\\nand 8), while other templates are relatively simpler (Table 12). However, we obtain\\npretty good performance on adddiv, despite it having a similar template.\\nSecond operation\\nFirst operation\\nadd\\nsub\\nmul\\ndiv\\nadd\\n1.0 卤 0\\n1.0 卤 0\\n0.01 卤 0.017\\n0.784 卤 0.112\\nsub\\n0.998 卤 0.004\\n0.998 卤 0.004\\n0.202 卤 0.247\\n0.224 卤 0.213\\nmul\\n0.996 卤 0.005\\n0.974 卤 0.047\\n0.998 卤 0.004\\n1.0 卤 0\\ndiv\\n0.93 卤 0.063\\n0.834 卤 0.288\\n0.99 卤 0.012\\n0.946 卤 0.11\\nTable 6: Generalization from one operation to two operations. Accuracy when\\ntraining on single-operation problems and testing on two-operation problems, presented for\\nall operation pairs.\\n4\\nDiscussion\\nThis paper introduces the concept of Modular Reasoning, Knowledge and Language\\n(MRKL) systems, which embraces large language models (LMs) and augments them\\nwith an easily extensible set of external knowledge and reasoning modules. This\\n铿exible, neuro-symbolic design retains all the advantages of modern LMs, but avoids\\ntheir limitations: a lack of current and/or proprietary information, and an inability\\nto reason symbolically when needed. We discussed some of the technical challenges\\nin implementing a MRKL system, with a special focus on how to cross the neuro-\\nsymbolic divide, which we did by looking at how Jurassic-X  AI21 Labs imple-\\nmentation of a MRKL system  was trained to handle basic arithmetic reliably.\\n13\\nA\\nFew-shot vs. prompt tuning\\nPerforming these experiments in a few-shot setting might be a more natural choice\\nthat requires less e铿ort in training these models. However, as the analysis below\\nreveals, the performance of this approach is limited, demonstrating the need to\\nfollow a systematic approach.\\nSet up:\\nIn the few-shot experiments, 10 examples were drawn from the training\\nsets and used as a prompt for the model. The performance was evaluated on the\\nsame test set for both the few-shot and the prompt-tuning approaches.\\nChanging the number of digits:\\nBoth methods easily generalize to varying the\\nnumber of digits (see Table 7).\\nNum. digits\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nPrompt tuning\\n1.0\\n1.0\\n1.0\\n1.0\\n1.0\\n1.0\\n1.0\\n1.0\\n1.0\\nFew-shot\\n1.0\\n1.0\\n1.0\\n1.0\\n1.0\\n1.0\\n1.0\\n1.0\\n1.0\\nTable 7:\\nNumber of digits: few-shot vs. prompt tuning. Results shown for addition.\\nWriting the operands in words instead of digits:\\nA clear di铿erence appears\\nas the few-shot approach is unable to handle this task and the performance greatly\\ndecreases with the number of digits (see Table 8).\\nNum. digits\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nPrompt tuning\\n1.0\\n1.0\\n1.0\\n0.95\\n0.99\\n1.0\\n0.98\\n0.99\\n0.98\\nFew-shot\\n1.0\\n1.0\\n0.93\\n0.62\\n0.6\\n0.38\\n0.17\\n0.03\\n0.0\\nTable 8:\\nNumber of digits: few-shot vs. prompt tuning, when numbers are\\nwritten as words. Results are shown for addition.\\nDi铿erent question formats:\\nWe explored the ability to generalize to di铿erent\\nquestion formats when training on one format (format 0). We note the generalization\\nis much lower compared to following our prompt-tuning method (Table 9).\\nNum. digits\\nFormat 0\\nFormat 1\\nFormat 2\\nFormat 3\\nFormat 4\\nPrompt tuning\\n1.0\\n1.0\\n1.0\\n1.0\\n0.993\\nFew-shot\\n1.0\\n1.0\\n0.98\\n0.78\\n0.22\\nTable 9:\\nGeneralization across formats: few-shot vs. prompt tuning. Results are\\nshown for addition.\\n14\\nB\\nQuestion formats\\nTable 10 shows the 铿ve formats used for single-operation problems for all operations.\\nFormat\\naddition\\nsubtraction\\n0\\nHow much is {x} plus {y}?\\nHow much is {x} minus {y}?\\n1\\nWhat is {x} plus {y}?\\nWhat is {x} minus {y}?\\n2\\nWhat is the result of {x} plus {y}?\\nWhat is the result of {x} minus {y}?\\n3\\nWhat is the sum of {x} and {y}?\\nWhat is the di铿erence between {x} and {y}?\\n4\\nThe sum of {x} and {y} is\\nThe di铿erence between {x} and {y} is\\nmultiplication\\ndivision\\n0\\nHow much is {x} times {y}?\\nHow much is {x} over {y}?\\n1\\nWhat is {x} times {y}?\\nWhat is {x} over {y}?\\n2\\nWhat is the result of {x} times {y}?\\nWhat is the result of {x} over {y}?\\n3\\nWhat is the product of {x} and {y}?\\nWhat is the ratio between {x} and {y}?\\n4\\nThe product of {x} and {y} is\\nThe ratio of {x} and {y} is\\nTable 10: Question formats for single-operation problems. Five di铿erent formats\\nfor each type of operation in the case of single-operation expressions.\\nThe equations for two-operation problems, for all combinations of placing the\\nbrackets, are shown in Table 11. Table 12 shows the phrasings for these 29 formulae.\\nC\\nAmount of data in each experiment\\nExperiment 1 - number of digits:\\nThe training set included 40 single digits operand combinations. The test set for\\nsingle digits included the remaining 41 combinations (discarding 0) and 50 randomly\\nchosen combinations for all the cases with more then 1 digit.\\nExperiment 2 - digits and words:\\n200 samples were randomly drawn for each number of digits (except for 1 and 2\\ndigits) and data was split equally between train and test data. This was repeated\\nfor both digits and words.\\nExperiment 3 - question formats:\\nThe train set included 400 samples with format 0 and the dev and test set each in-\\ncluded 200 samples from each format, randomly drawn with number of digits ranging\\nbetween 1 and 9.\\nExperiment 4 - generalization across operations:\\nThe train set included 635 samples across all formats and digits for each operation,\\nwhile the dev and test set each included 315 samples for each operation.\\nFor the two-operation experiment we drew 120 samples for each of the 29 formats,\\nwhich were divided equally between the train, dev and test sets.\\nExperiment 5 - generalization across the number of operations:\\n15\\n+\\n\\n\\n/\\n+\\nA + B + C\\nA + B C\\n(A + B) C\\nA + (B C)\\n(A + B)/C\\nA + (B/C)\\n\\n(A B) + C\\nA (B + C)\\n(A B) C\\nA (B C)\\n(A B) C\\nA (B C)\\n(A B)/C\\nA (B/C)\\n\\nA (B + C)\\n(A B) + C\\nA (B C)\\n(A B) C\\nA B C\\n(A B)/C\\nA (B/C)\\n/\\nA/(B + C)\\n(A/B) + C\\nA/(B C)\\n(A/B) C\\n(A/B) C\\nA/(B C)\\n(A/B)/C\\nA/(B/C)\\nTable 11: Two-operation formulae. All combinations of two-operation formulae, includ-\\ning operator precedence where relevant. See Table 12 for the phrasings corresponding to\\nthese formulae.\\n700 samples with a single operation were used for training the model, drawn ran-\\ndomly from all operations, formats, and number of digits. The dev and test set\\neach included 210 samples drawn randomly for each of the 16 combinations of two\\noperation formulas, analyzed up to 7 digits.\\n16\\nFormula\\nFormat\\nf=((A+B)*C)\\nSum A and B and multiply by C\\nf=(A+B*C)\\nWhat is the sum of A and the product of B and C?\\nf=((A-B)*C)\\nWhat is the product of A minus B and C?\\nf=(A/(B/C))\\nHow much is A divided by the ratio between B and C?\\nf=(A-B*C)\\nWhat is the di铿erence between A and the product of B and C?\\nf=(A*(B-C))\\nHow much is A times the di铿erence between B and C?\\nf=((A+B)/C)\\nWhat is the ratio between A plus B and C?\\nf=(A-(B-C))\\nHow much is A minus the di铿rence between B and C?\\nf=((A-B)/C)\\nWhat is the ratio between A minus B and C?\\nf=(A-B/C)\\nWhat is the di铿erence between A and the ratio between B and C?\\nf=(A/(B+C))\\nHow much is A divided bu the sum of B and C?\\nf=(A/(B-C))\\nHow much is A divided by the di铿erence between B and C?\\nf=(A+B/C)\\nwhat is the sum of A and the ratio between B and C?\\nf=(A*(B/C))\\nHow much is A times the ratio between B and C?\\nf=(A*B+C)\\nHow much is the sum of A times B and C?\\nf=(A*(B+C))\\nHow much is A times the sum of B and C?\\nf=(A/B+C)\\nHow much is the sum of A divided by B and C?\\nf=(A/B/C)\\nHow much is A divided by B divided by C?\\nf=(A/B-C)\\nHow much is the di铿erence between A divided by B and C?\\nf=(A/B*C)\\nHow much is A divided by B times C?\\nf=(A-(B+C))\\nHow much is A minus the sum of B and C?\\nf=(A*B-C)\\nHow much is the di铿erence between A times B and C?\\nf=(A/(B*C))\\nHow much is A divided by the product of B and C?\\nf=(A-B+C)\\nHow much is A minus B plus C?\\nf=(A+B+C)\\nHow much is A plus B plus C?\\nf=(A-B-C)\\nHow much is A minus B minus C?\\nf=(A*B/C)\\nHow much is A times B divided by C?\\nf=(A+B-C)\\nHow much is A plus B minus C?\\nf=(A*B*C)\\nHow much is A times B times C?\\nTable 12:\\nFormats of two-operation questions.\\n17\\nReferences\\n1.\\nDevlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: Pre-training of Deep\\nBidirectional Transformers for Language Understanding in Proceedings of the\\n2019 Conference of the North American Chapter of the Association for Compu-\\ntational Linguistics: Human Language Technologies, Volume 1 (Long and Short\\nPapers) (Association for Computational Linguistics, Minneapolis, Minnesota,\\nJune 2019), 41714186. https://aclanthology.org/N19-1423.\\n2.\\nBrown, T. B. et al. Language Models are Few-Shot Learners 2020. https :\\n//arxiv.org/abs/2005.14165.\\n3.\\nLieber, O., Sharir, O., Lenz, B. & Shoham, Y. Jurassic-1: Technical Details and\\nEvaluation 2021. https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/\\n61138924626a6981ee09caf6_jurassic_tech_paper.pdf.\\n4.\\nChowdhery, A. et al. PaLM: Scaling Language Modeling with Pathways 2022.\\nhttps://arxiv.org/abs/2204.02311.\\n5.\\nRa铿el, C. et al. Exploring the Limits of Transfer Learning with a Uni铿ed Text-\\nto-Text Transformer. Journal of Machine Learning Research 21, 167. http:\\n//jmlr.org/papers/v21/20-074.html (2020).\\n6.\\nSanh, V. et al. Multitask Prompted Training Enables Zero-Shot Task Generaliza-\\ntion in International Conference on Learning Representations (2022). https:\\n//openreview.net/forum?id=9Vrb9D0WI4.\\n7.\\nAribandi, V. et al. ExT5: Towards Extreme Multi-Task Scaling for Transfer\\nLearning in International Conference on Learning Representations (2022). https:\\n//openreview.net/forum?id=Vzh1BFUCiIX.\\n8.\\nLiu, Y. et al. RoBERTa: A Robustly Optimized BERT Pretraining Approach\\n2019. https://arxiv.org/abs/1907.11692.\\n9.\\nSmith, S. et al. Using DeepSpeed and Megatron to Train Megatron-Turing NLG\\n530B, A Large-Scale Generative Language Model 2022. https://arxiv.org/\\nabs/2201.11990.\\n10.\\nBommasani, R. et al. On the Opportunities and Risks of Foundation Models\\n2021. https://arxiv.org/abs/2108.07258.\\n11.\\nPetroni, F. et al. Language Models as Knowledge Bases? 2019. https://arxiv.\\norg/abs/1909.01066.\\n12.\\nJiang, Z., Xu, F. F., Araki, J. & Neubig, G. How Can We Know What Language\\nModels Know? 2019. https://arxiv.org/abs/1911.12543.\\n13.\\nTamkin, A., Brundage, M., Clark, J. & Ganguli, D. Understanding the Ca-\\npabilities, Limitations, and Societal Impact of Large Language Models 2021.\\nhttps://arxiv.org/abs/2102.02503.\\n18\\n14.\\nLoureiro, D., Barbieri, F., Neves, L., Anke, L. E. & Camacho-Collados, J.\\nTimeLMs: Diachronic Language Models from Twitter 2022. https://arxiv.\\norg/abs/2202.03829.\\n15.\\nWei, J. et al. Finetuned Language Models Are Zero-Shot Learners 2021. https:\\n//arxiv.org/abs/2109.01652.\\n16.\\nMin, S., Lewis, M., Zettlemoyer, L. & Hajishirzi, H. MetaICL: Learning to\\nLearn In Context 2021. https://arxiv.org/abs/2110.15943.\\n17.\\nSharir, O., Peleg, B. & Shoham, Y. The Cost of Training NLP Models: A\\nConcise Overview 2020. https://arxiv.org/abs/2004.08900.\\n18.\\nWu, C.-J. et al. Sustainable AI: Environmental Implications, Challenges and\\nOpportunities 2021. https://arxiv.org/abs/2111.00364.\\n19.\\nStrubell, E., Ganesh, A. & McCallum, A. Energy and Policy Considerations for\\nDeep Learning in NLP in Proceedings of the 57th Annual Meeting of the Asso-\\nciation for Computational Linguistics (Association for Computational Linguis-\\ntics, Florence, Italy, July 2019), 36453650. https://aclanthology.org/P19-\\n1355.\\n20.\\nWolfson, T. et al. Break It Down: A Question Understanding Benchmark 2020.\\nhttps://arxiv.org/abs/2001.11770.\\n21.\\nLester, B., Al-Rfou, R. & Constant, N. The Power of Scale for Parameter-\\nE铿cient Prompt Tuning 2021. https://arxiv.org/abs/2104.08691.\\n19\\n', metadata={'Published': '2022-05-01', 'Title': 'MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning', 'Authors': 'Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, Amnon Shashua, Moshe Tenenholtz', 'Summary': 'Huge language models (LMs) have ushered in a new era for AI, serving as a\\ngateway to natural-language-based knowledge tasks. Although an essential\\nelement of modern AI, LMs are also inherently limited in a number of ways. We\\ndiscuss these limitations and how they can be avoided by adopting a systems\\napproach. Conceptualizing the challenge as one that involves knowledge and\\nreasoning in addition to linguistic processing, we define a flexible\\narchitecture with multiple neural models, complemented by discrete knowledge\\nand reasoning modules. We describe this neuro-symbolic architecture, dubbed the\\nModular Reasoning, Knowledge and Language (MRKL, pronounced \"miracle\") system,\\nsome of the technical challenges in implementing it, and Jurassic-X, AI21 Labs\\'\\nMRKL system implementation.'})]\n",
            "[Document(page_content=' \\n \\n \\n \\n \\n \\nJuly 23, 2021  \\nCosmo Artificial Intelligence - AI Ltd  \\n Roger Gray  \\nVP Quality and Regulatory  \\nDonawa Lifescience Cons ulting Srl  \\nPiazza Albania 10  \\nRome, 00153  \\nItaly \\n \\nRe:  K211951  \\nTrade/Device Name:  GI Genius  \\nRegulation Number:  21 CFR 876.1520  \\nRegulation Name:  Gastrointestinal Lesion Software Detection System  \\nRegulatory Class:  Class II  \\nProduct Code:  QNP  \\nDated:  June 18, 2021  \\nReceived:  June 23, 2021  \\n \\nDear Roger Gray:  \\n \\nWe have reviewed your Section 510(k) premarket notification of intent to market the device referenced \\nabove and have determined the device is substantially equivalent (for the indications for use stated in the \\nenclosure) to legally marketed predicate devic es marketed in interstate commerce prior to May 28, 1976, the \\nenactment date of the Medical Device Amendments, or to devices that have been reclassified in accordance \\nwith the provisions of the Federal Food, Drug, and Cosmetic Act (Act) that do not require  approval of a \\npremarket approval application (PMA). You may, therefore, market the device, subject to the general \\ncontrols provisions of the Act. Although this letter refers to your product as a device, please be aware that \\nsome cleared products may inste ad be combination products. The 510(k) Premarket Notification Database \\nlocated at https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfpmn/pmn.cfm  identifies combination \\nproduct submissions. The general controls provisions of the Act include requirements for annual registration, \\nlisting of devices, good manufacturing practice, labeling, and prohibitions against misbranding and \\nadulteration. Please n ote:  CDRH does not evaluate information related to contract liability warranties. We \\nremind you, however, that device labeling must be truthful and not misleading.  \\n \\nIf your device is classified (see above) into either class II (Special Controls) or class III (PMA), it may be \\nsubject to additional controls. Existing major regulations affecting your device can be found in the Code of \\nFederal Regulations, Title 21, Parts 800 to 898. In addition, FDA may publish further announcements \\nconcerning your device in the Federal Register . \\n \\n', metadata={'source': '/content/K211951.pdf', 'page': 0}), Document(page_content='K211951 - Roger Gray  Page  \\n 2 \\nPlease be advised that FDA\\'s issuance of a substantial equivalence determination does not mean that FDA \\nhas made a determination that your device complies with other requirements of the Act or any Federal \\nstatutes and regulations adm inistered by other Federal agencies. You must comply with all the Act\\'s \\nrequirements, including, but not limited to: registration and listing (21 CFR Part 807); labeling (21 CFR Part \\n801); medical device reporting (reporting of medical device -related adver se events) (21 CFR 803) for \\ndevices or postmarketing safety reporting (21 CFR 4, Subpart B) for combination products (see \\nhttps://www.fda.gov/combination -products/guidance -regulatory -information/postmarketing -safety -reporting -\\ncombination -products ); good manufacturing practice requirements as set forth in the quality systems (QS) \\nregulation (21 CFR Part 820) for devices  or current good manufacturing practices (21 CFR 4, Subpart A) for \\ncombination products; and, if applicable, the electronic product radiation control provisions (Sections 531 -\\n542 of the Act); 21 CFR 1000 -1050.  \\n \\nAlso, please note the regulation entitled, \"M isbranding by reference to premarket notification\" (21 CFR Part \\n807.97). For questions regarding the reporting of adverse events under the MDR regulation (21 CFR Part \\n803), please go to https://www.fda.gov/medical -devices/medical -device -safety/medical -device -reporting -\\nmdr-how-report -medical -device -problems . \\n \\nFor comprehensive regulatory information about medical devices a nd radiation -emitting products, including \\ninformation about labeling regulations, please see Device Advice ( https://www.fda.gov/medical -\\ndevices/device -advice-comprehensive -regulatory -assistance ) and CDRH Learn \\n(https://www.fda.gov/training -and-continuing -education/cdrh -learn ). Additionally, you may contact the \\nDivision of Indus try and Consumer Education (DICE) to ask a question about a specific regulatory topic. See \\nthe DICE website ( https://www.fda.gov/medical -devices/device -advice -comprehensive -regulatory -\\nassistance/contact -us-division -industry -and-consumer -education -dice) for more information or contact DICE \\nby email ( DICE@fda.h hs.gov ) or phone (1 -800-638-2041 or 301 -796-7100).  \\n \\nSincerely,  \\n \\n \\n \\n \\nfor \\nShanil P. Haugen , Ph.D.  \\nAssistant Director  \\nDHT3A: Division of Renal,  Gastrointestinal,  \\n   Obesity and Transplant Devices  \\nOHT3: Office of GastroRenal, ObGyn,  \\n    General Hospital and Urology Devices  \\nOffice of Product Evaluation and Quality  \\nCenter for Devices and Radiological Health  \\n \\nEnclosure  \\n \\n ', metadata={'source': '/content/K211951.pdf', 'page': 1}), Document(page_content='DEPARTMENT OF HEALTH AND HUMAN SERVICES \\nFood and Drug Administration \\nIndications for Use Form Approved: OMB No. 0910-0120 \\nExpiration Date: 06/30/2023 \\nSee PRA Statement below. \\n510(k) Number (if known) \\nDevice Name \\nGI Genius \\nIndications for Use (Describe) \\nThe GI Genius System is a computer-assisted reading tool designed to aid endoscopists in detecting colonic mucosal \\nlesions (such as polyps and adenomas) in real time during standard white-light endoscopy examinations of patients \\nundergoing screening and surveillance endoscopic mucosal evaluations. The GI Genius computer-assisted detection \\ndevice is limited for use with standard white-light endoscopy imaging only. This device is not intended to replace clinical \\ndecision making. \\nType of Use (Select one or both, as applicable) \\n Prescription Use (Part 21 CFR 801 Subpart D) Over-The-Counter Use (21 CFR 801 Subpart C) \\nCONTINUE ON A SEPARATE PAGE IF NEEDED. \\nThis section applies only to requirements of the Paperwork Reduction Act of 1995. \\n*DO NOT SEND YOUR COMPLETED FORM TO THE PRA STAFF EMAIL ADDRESS BELOW.*\\nThe burden time for this collection of information is es timated to average 79 hours per response, including the \\ntime to review instructions, search existing data sources, gather and maintain the data needed and complete \\nand review the collection of information. Send comments regarding this burden estimate or any other aspect \\nof this information collection, including suggestions for reducing this burden, to: \\nDepartment of Health and Human Services \\nFood and Drug Administration \\nOffice of Chief Information Officer \\nPaperwork Reduction Act (PRA) Staff \\nPRAStaff@fda.hhs.gov \\nAn agency may not conduct or sponsor, and a person is not required to respond to, a collection of \\ninformation unless it displays a currently valid OMB number. \\nFORM FDA 3881 (6/20) Page 1 of 1 PSC Publishing Services (301) 443-6740 EF \\nK211951', metadata={'source': '/content/K211951.pdf', 'page': 2}), Document(page_content='P1 f 4\\n510(k) Summary \\n510(k) Reference: K211951  \\nDevice Name: GI Genius \\nType of 510(k) submission: Special \\nDate of submission: 18 June 2021  \\n510(k) Owner and Submitter:  Cosmo Artificial Intelligence - AI Ltd \\nRiverside II, Sir John Rogersons Quay \\nDublin D02 KV60 Ireland \\nFDA Establishment Reg. Number:  3018899987 \\nSpecification Developer: Linkverse Srl\\nvia Ostiense 131/L \\n00154 Rome \\nItaly \\nOwner/Operator Reg. Number: 3018901422 \\n510(k) Application Correspondent:  Roger Gray \\nVP Quality and Regulatory \\nDonawa Lifescience Consulting Piazza Albania 10 \\n00153 Rome, Italy \\nPhone: +39 06 578 2665\\nFax: +39 06 574 3786\\nEmail:   rgray@donawa.com\\nFDA Product Code:   QNP \\nFDA Regulation Number:  21 CFR 876.1520 \\nFDA Classification Name:  Gastrointestinal lesion software detection system \\nClassification Panel:  Gastroenterology and Urology \\nFDA Classification:   Class II \\nIndications for Use: The GI Genius System is a computer-assisted reading tool designed to aid endoscopists in detecting colonic mucosal lesions (such as polyps and adenom as) in real time during standard white-light \\nendoscopy examinations of patients undergoing screening and surveillance endoscopic mucosal \\nevaluations. The GI Genius computer-assisted detection device is limited for use with standard white-light \\nendoscopy imaging only. This device is not in tended to replace clinical decision making. K211951\\nPage 1 of 4', metadata={'source': '/content/K211951.pdf', 'page': 3}), Document(page_content='P2 f 4\\nThe indications for use statement is identical to that  for the original unmodified (predicate) device, as \\ncleared under DEN200055. \\nDevice Description: \\nGI Genius is an artificial intelligence-based device th at has been trained to process colonoscopy images \\ncontaining regions consistent with colorectal lesions li ke polyps, including those with flat (non-polypoid) \\nmorphology. \\nGI Genius is compatible with the following Video Processors : Fujifilm VP-7000, Olympus CV-180 EXERA II, \\nOlympus CV-190 EXERA III, Fujifilm VP-4450HD, and Pentax EPK-i7000 Video Processor . \\nGI Genius is connected between the video processor and the endoscopic display monitor. When first \\nswitched on, the endoscopic field of view is clearly identified by four corner markers, and a blinking green square indicator appears on the connected endoscopic display monitor to state that the system is ready to function. \\nDuring live video streaming of the endoscopic video image, GI Genius generates a video output on the \\nendoscopic display monitor that contains the original  live video together with superimposed green square \\nmarkers that will appear when a polyp or other lesion of interest is detected, accompanied by a short sound. These markers will not be visible when no lesion detection occurs. \\nThe operating principle of the subject device is identical to that of the predicate device, this being a \\ncomputer-assisted detection device used in conjunction with endoscopy for the detection of abnormal \\nlesions in the gastrointestinal tract. This device with advanced software algorithms brings attention to images to aid in the detection of lesions. The devi ce includes hardware to support interfacing with video \\nendoscopy systems. \\nDesign changes: \\nThis Special 510(k) describes the design changes inco rporated into GI Genius following its FDA clearance \\nunder DEN200055. The device software version number as cleared under DEN 2000555 was 1.0.2; the \\ndevice software version that is the subject of this Special 510(k) is 2.0.0. \\nThe design changes include a single software design change that falls within the FDA guideline for \\nsubmittal of a new 510(k). A labeling change is also planned, to add non-clinical performance data of the \\nnew software version and to include one additional video processor type in the list of compatible units in \\nthe User Manual. Other minor design changes have been the subject of internal documentation and are \\nbeing advised to FDA for information only. \\nThe software design change includes an upgrade to th e inference engine to optimize algorithm speed and \\nimprove energy efficiency, together with a retraining of the neural network with additional procedure \\nvideos, and improved data augmentation, to improve polyp detection capability robustness. \\nNon-clinical testing: \\nThe following non-clinical verification/ validation activities have been completed: \\nxVerification of the revised software at the system level. Each element of the SRS was tested and found\\nto meet specified requirements, testing 14 Units and 9 Items, encompassing 133 software\\nrequirements.K211951\\nPage 2 of 4', metadata={'source': '/content/K211951.pdf', 'page': 4}), Document(page_content='P3 f 4\\nxValidation of the revised software at the user level, testing 14 Units and 9 Items, encompassing 133\\nsoftware requirements.\\nxProtective measures identified by risk management ha ve been verified during Installation Qualification\\nand Operational Qualification.\\nxElectromagnetic Compatibility (EMC) and Electrical Safety compliance tests have been completedaccording to IEC 60601-1 and IEC 60601-1-2 requirements.\\nxStandalone Performance Testing has been carried ou t to assess the performance of the subject device\\nin accordance with the same test protocol as that used for the predicate device, the results of which\\ndemonstrate substantial equivalence to the predicate device.\\nxNon-inferiority of performance of GI Genius with a new video processor has been established bymeans of a per-frame assessment on 40 pre-recorded procedures.\\nThe results of the above testing aid in demonstration of  substantial equivalence of the subject device with \\nthe predicate device, as the same test protocols have been used. \\nSubstantial equivalence \\nThe predicate device for the subject device is the pre- modification version of the same device, GI Genius, \\nFDA-cleared under DEN200055 on 9 April 2021: \\nPredicate Device: GI Genius Sponsor: Cosmo Artificial Intelligence - AI Ltd \\nDe Novo Number: DEN200055 \\nClearance Date:  9 April 2021 FDA Product Code: QNP \\nClassification Name: Gastrointestinal lesion software detection system Regulation No: 21 CFR 876.1520 \\nClass:   II \\nPredicate device comparison table: \\nTable 1 provides evidence of substantial equivalenc e of the subject device with the predicate device. \\nTable 1: Predicate device comparison table \\nCharacteristic Subject device Predicate device Comparison \\nDevice name GI Genius v.2.0.0 GI Genus v.1.0.2 N/A \\nManufacturer Linkverse Srl, Italy Linkverse Srl, Italy Same \\nFDA clearance K211951 DEN200055 N/A \\nFDA Reg name Gastrointestinal lesion software \\ndetection system Gastrointestinal lesion software detection system Same \\nFDA Reg # 21 CFR 876.1520 21 CFR 876.1520 Same \\nFDA Product Code QNP QNP Same K211951\\nPage 3 of 4', metadata={'source': '/content/K211951.pdf', 'page': 5}), Document(page_content='P4 f 4\\nTable 1: Predicate device comparison table \\nCharacteristic Subject device Predicate device Comparison\\nIndications for Use The GI Genius System is a \\ncomputer-assisted reading tool \\ndesigned to aid endoscopists in detecting colonic mucosal \\nlesions (such as polyps and \\nadenomas) in real time during standard white-light endoscopy examinations of patients \\nundergoing screening and \\nsurveillance endoscopic mucosal evaluations. The GI Genius computer-assisted detection \\ndevice is limited for use with \\nstandard white-light endoscopy imaging only. This device is not \\nintended to replace clinical \\ndecision making. The GI Genius System is a \\ncomputer-assisted reading tool \\ndesigned to aid endoscopists in detecting colonic mucosal \\nlesions (such as polyps and \\nadenomas) in real time during standard white-light endoscopy examinations of patients \\nundergoing screening and \\nsurveillance endoscopic mucosal evaluations. The GI Genius computer-assisted detection \\ndevice is limited for use with \\nstandard white-light endoscopy imaging only. This device is not \\nintended to replace clinical \\ndecision making. Same \\nVideo delay, signal in \\nto signal out 1.52 渭s 1.52渭s Same\\nLesion-based \\nsensitivity 86.5 % 82.0 % Improved \\nperformance \\nFrame level \\nperformance  (150 videos / 338 \\npolyps) True positive: 269,223  \\nTrue negative: 5,239,128  \\nFalse positive: 104,669  \\nFalse negative: 192,567  True positive: 228,929  \\nTrue negative: 5,235,682  \\nFalse positive: 108,115  \\nFalse negative:  232,861  Improved \\nperformance \\nTrue positive rate per \\nframe Mean: 58.30 % \\n% of polyps: 100 % Mean: 49.57 % % of polyps: 99.7 % Improved \\nperformance \\nFalse positive rate per frame Mean: 1.96 % Mean: 2.02 % Improved \\nperformance \\nFrame-Based TPr/FPr \\nROC curve, AOC 0.796 0.723Improv ed \\nperformance \\nFalse positive clusters \\nper patient < 500 ms: 40 less than baseline > 500 ms: 1 more than baseline< 500 ms: Baseline > 500 ms: BaselineImproved \\nSimilar \\nAdditional video processor Yes: Fujifilm VP-7000 N/A Improved \\nperformance \\nElectrical safety IEC 60601-1 IEC 60601-1 Same \\nElectromagnetic compatibility IEC 60601-1-2 IEC 60601-1-2 Same \\nLAN port Yes, non-functional to user No Different \\nThe subject device and the predicate device have many  identical or similar characteristics. None of the \\nidentified differences introduce new as pects of safety or effectiveness. \\nConclusion \\nThe subject and predicate devices have identical indications for use and fundamental technological characteristics. Any differences in performance between the subject and predicate devices do not raise \\ndifferent questions of safety and effectiveness. The performance data demonstrate that the subject \\ndevice is substantially equivalent to the predicate  device, which is already in interstate commerce \\nwithin the USA. Therefore, the subject device is as safe, as effective, and performs better than the \\npredicate device.K211951\\nPage 4 of 4', metadata={'source': '/content/K211951.pdf', 'page': 6})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S-3FBdT_lhVT",
      "metadata": {
        "id": "S-3FBdT_lhVT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "d6f1282f-54a6-45d5-e728-d28aab1f8527"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Documents\n",
            "Chunking Documents\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mAvailable Documents:\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - \u001b[0m\u001b[1;35m/content/\u001b[0m\u001b[1;95mK211951.pdf\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - \u001b[0m\u001b[1;35m/content/\u001b[0m\u001b[1;95mK152733.pdf\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - \u001b[0m\u001b[1;35m/content/\u001b[0m\u001b[1;95mK182149.pdf\u001b[0m\u001b[1;38;2;118;185;0m \u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Available Documents:</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">/content/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">K211951.pdf</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">/content/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">K152733.pdf</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">/content/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">K182149.pdf</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> </span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 0\n",
            " - Metadata: {'source': '/content/K211951.pdf', 'page': 0}\n",
            " - # Chunks: 20\n",
            "\n",
            "Document 1\n",
            " - Metadata: {'source': '/content/K152733.pdf', 'page': 0}\n",
            " - # Chunks: 49\n",
            "\n",
            "Document 2\n",
            " - Metadata: {'source': '/content/K182149.pdf', 'page': 0}\n",
            " - # Chunks: 35\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import ArxivLoader\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=100,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
        ")\n",
        "\n",
        "## TODO: Please pick some papers and add them to the list as you'd like\n",
        "## NOTE: To re-use for the final assessment, make sure at least one paper is < 1 month old\n",
        "print(\"Loading Documents\")\n",
        "\n",
        "docs = [\n",
        "    #ArxivLoader(query=\"1706.03762\").load(),  ## Attention Is All You Need Paper\n",
        "    #ArxivLoader(query=\"1810.04805\").load(),  ## BERT Paper\n",
        "    #ArxivLoader(query=\"2005.11401\").load(),  ## RAG Paper\n",
        "    #ArxivLoader(query=\"2205.00445\").load(),  ## MRKL Paper\n",
        "    #ArxivLoader(query=\"2310.06825\").load(),  ## Mistral Paper\n",
        "    #ArxivLoader(query=\"2306.05685\").load(),  ## LLM-as-a-Judge\n",
        "    ## Some longer papers\n",
        "    # ArxivLoader(query=\"2210.03629\").load(),  ## ReAct Paper\n",
        "    # ArxivLoader(query=\"2112.10752\").load(),  ## Latent Stable Diffusion Paper\n",
        "    # ArxivLoader(query=\"2103.00020\").load(),  ## CLIP Paper\n",
        "    ## TODO: Feel free to add more\n",
        "    PyPDFLoader(\"/content/K211951.pdf\").load(),\n",
        "    PyPDFLoader(\"/content/K152733.pdf\").load(),\n",
        "    PyPDFLoader(\"/content/K182149.pdf\").load(),\n",
        "\n",
        "]\n",
        "\n",
        "## Cut the paper short if references is included.\n",
        "## This is a standard string in papers.\n",
        "for doc in docs:\n",
        "    content = doc[0].page_content\n",
        "    if \"References\" in content:\n",
        "        doc[0].page_content = content[:content.index(\"References\")]\n",
        "\n",
        "## Split the documents and also filter out stubs (overly short chunks)\n",
        "print(\"Chunking Documents\")\n",
        "docs_chunks = [text_splitter.split_documents(doc) for doc in docs]\n",
        "docs_chunks = [[c for c in dchunks if len(c.page_content) > 200] for dchunks in docs_chunks]\n",
        "\n",
        "## Make some custom Chunks to give big-picture details\n",
        "doc_string = \"Available Documents:\"\n",
        "doc_metadata = []\n",
        "for chunks in docs_chunks:\n",
        "    metadata = getattr(chunks[0], 'metadata', {})\n",
        "    # doc_string += \"\\n - \" + metadata.get('Title') #for Arxiv loader metadata\n",
        "    doc_string += \"\\n - \" + metadata.get('source', 'Unknown')  # For PyPDFloader. Using 'source' as the title, with a fallback to 'Unknown'\n",
        "    doc_metadata += [str(metadata)]\n",
        "\n",
        "extra_chunks = [doc_string] + doc_metadata\n",
        "\n",
        "## Printing out some summary information for reference\n",
        "pprint(doc_string, '\\n')\n",
        "for i, chunks in enumerate(docs_chunks):\n",
        "    print(f\"Document {i}\")\n",
        "    print(f\" - Metadata: {chunks[0].metadata}\")\n",
        "    print(f\" - # Chunks: {len(chunks)}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M9AngCDmV2H7"
      },
      "id": "M9AngCDmV2H7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4pWU_OOnnrsT",
      "metadata": {
        "id": "4pWU_OOnnrsT"
      },
      "source": [
        "<br>\n",
        "\n",
        "### **Task 2**: Construct Your Document Vector Stores\n",
        "\n",
        "Now that we have all of the components, we can go ahead and create indices surrounding them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lwwmr3aptwCg",
      "metadata": {
        "id": "lwwmr3aptwCg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea18d058-fb68-41a0-9293-a734a5f97bba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constructing Vector Stores\n",
            "CPU times: user 623 ms, sys: 20.1 ms, total: 643 ms\n",
            "Wall time: 6.29 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "## ^^ This cell will output a time\n",
        "from faiss import IndexFlatL2\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "embedder = NVIDIAEmbeddings(model=\"nvolveqa_40k\", model_type=None)\n",
        "\n",
        "## Construct series of document vector stores\n",
        "print(\"Constructing Vector Stores\")\n",
        "vecstores = [FAISS.from_texts(extra_chunks, embedder)]\n",
        "vecstores += [FAISS.from_documents(doc_chunks, embedder) for doc_chunks in docs_chunks]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j39JwCKubto0",
      "metadata": {
        "id": "j39JwCKubto0"
      },
      "source": [
        "<br>\n",
        "\n",
        "From there, we can combine our indices into a single one using the following utility:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q7us66iPVc70",
      "metadata": {
        "id": "Q7us66iPVc70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e330e692-fe33-4a76-9d04-b40dfb238f15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constructed aggregate docstore with 108 chunks\n"
          ]
        }
      ],
      "source": [
        "embed_dims = len(embedder.embed_query(\"test\"))\n",
        "def default_FAISS():\n",
        "    '''Useful utility for making an empty FAISS vectorstore'''\n",
        "    return FAISS(\n",
        "        embedding_function=embedder,\n",
        "        index=IndexFlatL2(embed_dims),\n",
        "        docstore=InMemoryDocstore(),\n",
        "        index_to_docstore_id={},\n",
        "        normalize_L2=False\n",
        "    )\n",
        "\n",
        "def aggregate_vstores(vectorstores):\n",
        "    ## Initialize an empty FAISS Index and merge others into it\n",
        "    ## We'll use default_faiss for simplicity, though it's tied to your embedder by reference\n",
        "    agg_vstore = default_FAISS()\n",
        "    for vstore in vectorstores:\n",
        "        agg_vstore.merge_from(vstore)\n",
        "    return agg_vstore\n",
        "\n",
        "if 'docstore' not in globals():\n",
        "    ## Unintuitive optimization; merge_from seems to optimize constituent vector stores away\n",
        "    docstore = aggregate_vstores(vecstores)\n",
        "\n",
        "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VU_VEx2mqJUK",
      "metadata": {
        "id": "VU_VEx2mqJUK"
      },
      "source": [
        "<br>\n",
        "\n",
        "### **Task 3: [Exercise]** Implement Your RAG Chain\n",
        "\n",
        "Finally, all the puzzle pieces are in place to implement the RAG pipeline! As a review, we now have:\n",
        "\n",
        "- A way to construct a from-scratch vector store for conversational memory (and a way to initialize an empty one with `default_FAISS()`)\n",
        "\n",
        "- A vector store pre-loaded with useful document information from our `ArxivLoader` utility (stored in `docstore`).\n",
        "\n",
        "With the help of a couple more utilities, you're finally ready to integrate your chain! A few additional convenience utilities are provided (`doc2str` and the now-common `RPrint`) but are optional to use. Additionally, some starter prompts and structures are also defined.\n",
        "\n",
        "> **Given all of this:** Please implement the `retrieval_chain`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-RXSrb1GcNff",
      "metadata": {
        "id": "-RXSrb1GcNff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c95c1fae-b6f8-4a3e-fa12-4a74822d9b2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input': '\\nIgnore the conversation history.\\nFor document K152733, extract the following information into the provided summary format{\\n\\nSummary format{\\nDevice Name: (Device Name)\\nDate: (MM DD, YYYY)\\nRegulatory Class: (Class ?)\\nPredicates/Predicate Devices: (Predicate Device)\\nMethodology and intended use: (One paragraph)\\nClinical study details:\\nRetrospective or Prospective: (regions:?, sites:?, hospitals:?)\\nSample Size with subgroups: (20 words at most)\\nData Type: (5 words at most) (CT, US, MRI,ECG, etc..)\\nGround truth/reference standard details: (30 words at most with annotated data statistics and annotator/reader statistics)\\nStudy endpoint details: (40 words at most)\\nPerformance results: (20 words at most) \\n}\\n}\\n', 'history': '', 'context': '[Quote from Document] arteries. The results of HeartFlow FFRCT are intended to be used by qualified clinicians in conjunction with the patients clinical history, symptoms, and other diagnostic tests, as well as the clinicians professional judgment. \\nType of Use (Select one or both, as applicable)\\nPrescription Use (Part 21 CFR 801 Subpart D) Over-The-Counter Use (21 CFR 801 Subpart C) \\nCONTINUE ON A SEPARATE PAGE IF NEEDED. \\nThis section applies only to requirements of the Paperwork Reduction Act of 1995.\\n*DO NOT SEND YOUR COMPLETED FORM TO THE PRA STAFF EMAIL ADDRESS BELOW.*\\nThe burden time for this collection of information is estimated to average 79 hours per response, including the \\ntime to review instructions, search existing data sources, gather and maintain the data needed and completeand review the collection of information. Send comments regarding this burden estimate or any other aspectof this information collection, including suggestions for reducing this burden, to:\\n[Quote from Document] 510(k) Summa ry  K182149  \\n \\nCathWorks FFR angio Traditional 510(k)   Page -8 of 8 Special Controls  (abbreviated from \\nregulation)  How Fulfilled  \\n(iii)  Key assum ptions  \\n(iv)  The measurement performance of the \\ndevice for all presented parameters  \\n(v)  A detailed description of the clinical \\nstudy subje cts and results  \\n(vi)  A detailed description of the analysis \\nprocedure using the device and any data \\nfeatures that could affect accuracy of \\nresults.  population  \\n Appr opriate warni ngs and precautions for \\nsafe use of the device , including  factors that \\ncould adversel y affect the device output and \\ncautions to use the device outpu t in context \\nwith other clinical factors for patient care  \\n Limitati ons describing patient pop ulations \\nand lesion t ypes for which the safety and \\neffectiveness of FFRangio has not been \\nevalua ted \\n Instructions  for use providing guidelines for \\nthe compatib le image acquisition systems \\nand the r equired criteria for the images to\\n[Quote from Document] enactment date of the Medical Device Amendments, or  to devices that have been reclassified in accordance \\nwith the provisions of the Federal Food, Drug, and Cosmeti c Act (Act) that do not require approval of a \\npremarket approval application (PMA). You may, therefore, ma rket the device, subject to the general \\ncontrols provisions of the Act. Although thi s letter refers to your product as a device, please be aware that \\nsome cleared products may instead be com bination products. The 510(k) Premarket Notification Database \\nlocated at https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfpmn/pmn.cfm  identifies combination \\nproduct submissions. The general controls provisions of  the Act include re quirements for annual registration, \\nlisting of devices, good manufacturing practice, labeling, and prohibitions against misbranding and \\nadulteration. Please note:  CDRH does not evaluate inf ormation related to contract liability warranties. We\\n[Quote from Document] 510(k) Summa ry K182149  \\nCathWorks FFR angio Traditional 510(k)  Page -1 of 8 510(k) Summary  \\nThis summary of 510(k) safety and effectiveness information is being submitted per the \\nrequirements of 21 CFR 807.92.  \\nA. Submitter : Heyer Regulatory Solutions LLC  \\nP.O. Box 2151  \\nAmherst, MA 01004 -2151  \\nContact:  Sheila Hemeon -Heyer  \\nSheila@heyer -regulatory.com   \\nB. Manufacturer  Contact:  CathWorks, Ltd.  \\nIlanit Frank Hakim  \\n3 Rappaport St.  \\nKfar Saba 4465141, ISRAEL  \\nTel:  +972 9 7467387  \\nilanit@cath.wo rks \\nC. Date Prepared : December 19, 2018 \\nD. Device Name and Classification Information : \\nTrade Name:  \\nClassification Name:  \\nCommon Name:  \\nRegulation:  \\nProd uct Code : \\nReview Panel : \\nClass:  FFR angio System\\nCoronary Vascul ar Physiologi c Simulation Software \\nDevice \\nDigital FFR System \\n21 CFR 870.1415 \\nQEK \\nCardiovascular \\nII \\nE. Predicate Device (s): K161772  for FFR CT, manufactured by HeartFlow, Inc.  \\nF. Summary Device Description:\\n'}\n",
            "Based on the provided document, K152733, here is the extracted information in the\n",
            "  requested summary format:\n",
            "\n",
            "Device Name: FFR angio System\n",
            "Date: Not explicitly mentioned in the document\n",
            "Regulatory Class: Class II\n",
            "Predicates/Predicate Devices: K161772 for FFR CT, manufactured by HeartFlow, Inc.\n",
            "\n",
            "Methodology and intended use: The FFR angio System is a Digital FFR System, classified\n",
            "  under 21 CFR 870.1415, and falls under the Cardiovascular category. It is intended to be\n",
            "  used by qualified clinicians in conjunction with the patients clinical history,\n",
            "  symptoms, and other diagnostic tests, as well as the clinicians professional judgment.\n",
            "\n",
            "Clinical study details:\n",
            "- Retrospective or Prospective: Not explicitly mentioned in the document\n",
            "- Regions, sites, and hospitals: Also not explicitly mentioned\n",
            "- Sample Size with subgroups: No specific sample size or subgroups are mentioned\n",
            "- Data Type: The document does not explicitly mention the data type used in the study\n",
            "\n",
            "Ground truth/reference standard details: Not explicitly mentioned in the document\n",
            "\n",
            "Study endpoint details: The document does not provide specific study endpoint details\n",
            "\n",
            "Performance results: The performance results of the FFR angio System are not provided\n",
            "  in the document.\n",
            "\n",
            "Please note that the document primarily focuses on regulatory information and does not\n",
            "  contain comprehensive details about the clinical study, sample size, data type, or\n",
            "  performance results."
          ]
        }
      ],
      "source": [
        "from langchain.document_transformers import LongContextReorder\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.runnables.passthrough import RunnableAssign\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "import gradio as gr\n",
        "from functools import partial\n",
        "from operator import itemgetter\n",
        "\n",
        "#from langchain_community.llms import HuggingFaceHub\n",
        "\n",
        "\n",
        "llm = ChatNVIDIA(model=\"mixtral_8x7b\") | StrOutputParser()\n",
        "#llm = HuggingFaceHub(model=\"mixtral_8x7b\") | StrOutputParser()\n",
        "convstore = default_FAISS()\n",
        "\n",
        "def save_memory_and_get_output(d, vstore):\n",
        "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
        "    vstore.add_texts([\n",
        "        f\"User previously responded with {d.get('input')}\",\n",
        "        f\"Agent previously responded with {d.get('output')}\"\n",
        "    ])\n",
        "    return d.get('output')\n",
        "\n",
        "initial_msg = (\n",
        "    \"Hello! I am a document chat agent here to help the user!\"\n",
        "    f\" I have access to the following documents: {doc_string}\\n\\nHow can I help you?\"\n",
        ")\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
        "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
        "    \" User messaged just asked: {input}\\n\\n\"\n",
        "    \" From this, we have retrieved the following potentially-useful info: \"\n",
        "    \" Conversation History Retrieval:\\n{history}\\n\\n\"\n",
        "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
        "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)\"\n",
        "), ('user', '{input}')])\n",
        "\n",
        "################################################################################################\n",
        "## BEGIN TODO: Implement the retrieval chain to make your system work!\n",
        "\n",
        "retrieval_chain = (\n",
        "    {'input' : (lambda x: x)}\n",
        "    ## TODO: Make sure to retrieve history & context from convstore & docstore, respectively.\n",
        "    ## HINT: Our solution uses RunnableAssign, itemgetter, long_reorder, and docs2str\n",
        "    | RunnableAssign({'history' : itemgetter('input') | convstore.as_retriever() | long_reorder | docs2str}) #Uncomment to enable conversation history\n",
        "    | RunnableAssign({'context' : itemgetter('input') | docstore.as_retriever()  | long_reorder | docs2str})\n",
        "    | RPrint()\n",
        ")\n",
        "\n",
        "## END TODO\n",
        "################################################################################################\n",
        "\n",
        "stream_chain = chat_prompt | llm\n",
        "\n",
        "def chat_gen(message, history=[], return_buffer=True):\n",
        "    buffer = \"\"\n",
        "    ## First perform the retrieval based on the input message\n",
        "    retrieval = retrieval_chain.invoke(message)\n",
        "    line_buffer = \"\"\n",
        "\n",
        "    ## Then, stream the results of the stream_chain\n",
        "    for token in stream_chain.stream(retrieval):\n",
        "        buffer += token\n",
        "        ## If you're using standard print, keep line from getting too long\n",
        "        if not return_buffer:\n",
        "            line_buffer += token\n",
        "            if \"\\n\" in line_buffer:\n",
        "                line_buffer = \"\"\n",
        "            if ((len(line_buffer)>84 and token and token[0] == \" \") or len(line_buffer)>100):\n",
        "                line_buffer = \"\"\n",
        "                yield \"\\n\"\n",
        "                token = \"  \" + token.lstrip()\n",
        "        yield buffer if return_buffer else token\n",
        "\n",
        "    ## Lastly, save the chat exchange to the conversation memory buffer\n",
        "    save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)\n",
        "\n",
        "\n",
        "## Start of Agent Event Loop\n",
        "test_question = \"\"\"\n",
        "Summarize the documents.\n",
        "\"\"\"\n",
        "\n",
        "## Before you launch your gradio interface, make sure your thing works\n",
        "for response in chat_gen(test_question, return_buffer=False):\n",
        "    print(response, end='')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9W7sC5Z6BfqM",
      "metadata": {
        "id": "9W7sC5Z6BfqM"
      },
      "source": [
        "### **Task 4:** Interact With Your Gradio Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fMP3l7QL2JWT",
      "metadata": {
        "id": "fMP3l7QL2JWT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "outputId": "5120d7d4-cd8e-4e09-c1a0-8ea593f99590"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://2d22b00102d643d55c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2d22b00102d643d55c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input': 'Ignore the conversation history.\\nFor document K152733, extract the following information into the provided summary format, being concise like the provided example of a different product{\\n\\nSummary format{\\nDevice Name: (Device Name)\\nDate: (MM DD, YYYY)\\nRegulatory Class: (Class ?)\\nPredicates/Predicate Devices: (Predicate Device)\\nMethodology and intended use: (One paragraph)\\nClinical study details:\\nRetrospective or Prospective: (regions:?, sites:?, hospitals:?)\\nSample Size with subgroups: (20 words at most)\\nData Type: (5 words at most) (CT, US, MRI,ECG, etc..)\\nGround truth/reference standard details: (30 words at most with annotated data statistics and annotator/reader statistics)\\nStudy endpoint details: (40 words at most)\\nPerformance results: (20 words at most) \\n}\\n\\nHere is an example summary from a different product{\\n\\nDevice Name: FFRangio System, K182149\\nDate: December 19, 2022\\nRegulatory Class: Class II\\nPredicates/Predicate Devices: HeartFlow FFRCT V2.0  510(k) K161772\\nMethodology and intended use: The user selects the images and, following the system prompts, marks key features on the images including the target lesion, ostium location, main vessel, target vessel, and its side branches. The system then matches the corresponding vessels among the projections and generates a 3D computer model of the vessels. The 3D model is used for blood flow analysis and determination of the FFRangio.\\nClinical study details:\\nRetrospective or Prospective: Prospective; five countries from US, Europa and Israel; 10 sites/hospitals.\\nSample Size with subgroups: Sample size: 301 subjects, 319 coronary lesions.\\nData Type: X-Ray angiography.\\nGround truth/reference standard details:FFR.\\nStudy endpoint details: 70% sensitivity and  75% specificity for lower bounds of the 95% confidence intervals\\nPerformance results: 93.5% sensitivity with a lower one-sided 95% CI of 87.8%,  91.2% specificity with a lower one-sided bound 95% CI of 86.0%.) \\n}\\n}', 'history': '[Quote from Document] Agent previously responded with Based on the provided document K152733, here is the summary in the requested format:\\n\\nDevice Name: CathWorks FFRangio\\nDate: December 19, 2018\\nRegulatory Class: Class II\\nPredicates/Predicate Devices: K161772 for FFR CT, manufactured by HeartFlow, Inc.\\n\\nMethodology and intended use: The CathWorks FFRangio is a software device for the clinical quantitative and qualitative analysis of previously acquired angiography DICOM data for patients with coronary artery disease. It provides FFRangio, a mathematically derived quantity, computed from simulated blood flow information obtained from a 3D computer model, generated from coronary angiography images. FFRangio analysis is intended to support the functional evaluation of coronary artery disease. The results of this analysis are provided as a supportive aid for qualified clinicians in the evaluation and assessment of coronary arteries physiology. The results of CathWorks FFRangio are intended to be used by qualified clinicians in conjunction with the patients clinical history, symptoms, and other diagnostic tests, as well as the clinicians professional evaluation.\\n\\nClinical study details:\\nRegions: Not specified in the document\\nSites/Hospitals: Not specified in the document\\nRetrospective or Prospective: Not specified in the document\\nSample Size with subgroups: Not specified in the document\\nData Type: Coronary angiography images\\nGround truth/reference standard details: Not specified in the document\\nStudy endpoint details: Not specified in the document\\nPerformance results: Not specified in the document\\n\\nPlease note that the clinical study details, ground truth/reference standard details, study endpoint details, and performance results are not provided in the document K152733.\\n[Quote from Document] User previously responded with \\nIgnore the conversation history.\\nFor document K152733, extract the following information into the provided summary format, being concise like the provided example of a different product{\\n\\nSummary format{\\nDevice Name: (Device Name)\\nDate: (MM DD, YYYY)\\nRegulatory Class: (Class ?)\\nPredicates/Predicate Devices: (Predicate Device)\\nMethodology and intended use: (One paragraph)\\nClinical study details:\\nRetrospective or Prospective: (regions:?, sites:?, hospitals:?)\\nSample Size with subgroups: (20 words at most)\\nData Type: (5 words at most) (CT, US, MRI,ECG, etc..)\\nGround truth/reference standard details: (30 words at most with annotated data statistics and annotator/reader statistics)\\nStudy endpoint details: (40 words at most)\\nPerformance results: (20 words at most) \\n}\\n\\nHere is an example summary from a different product{\\n\\nDevice Name: FFRangio System, K182149\\nDate: December 19, 2022\\nRegulatory Class: Class II\\nPredicates/Predicate Devices: HeartFlow FFRCT V2.0  510(k) K161772\\nMethodology and intended use: The user selects the images and, following the system prompts, marks key features on the images including the target lesion, ostium location, main vessel, target vessel, and its side branches. The system then matches the corresponding vessels among the projections and generates a 3D computer model of the vessels. The 3D model is used for blood flow analysis and determination of the FFRangio.\\nClinical study details:\\nRetrospective or Prospective: Prospective; five countries from US, Europa and Israel; 10 sites/hospitals.\\nSample Size with subgroups: Sample size: 301 subjects, 319 coronary lesions.\\nData Type: X-Ray angiography.\\nGround truth/reference standard details:FFR.\\nStudy endpoint details: 70% sensitivity and  75% specificity for lower bounds of the 95% confidence intervals\\nPerformance results: 93.5% sensitivity with a lower one-sided 95% CI of 87.8%,  91.2% specificity with a lower one-sided bound 95% CI of 86.0%.) \\n}\\n}\\n\\n', 'context': '[Quote from Document] 510(k) Summa ry K182149  \\nCathWorks FFR angio Traditional 510(k)  Page -1 of 8 510(k) Summary  \\nThis summary of 510(k) safety and effectiveness information is being submitted per the \\nrequirements of 21 CFR 807.92.  \\nA. Submitter : Heyer Regulatory Solutions LLC  \\nP.O. Box 2151  \\nAmherst, MA 01004 -2151  \\nContact:  Sheila Hemeon -Heyer  \\nSheila@heyer -regulatory.com   \\nB. Manufacturer  Contact:  CathWorks, Ltd.  \\nIlanit Frank Hakim  \\n3 Rappaport St.  \\nKfar Saba 4465141, ISRAEL  \\nTel:  +972 9 7467387  \\nilanit@cath.wo rks \\nC. Date Prepared : December 19, 2018 \\nD. Device Name and Classification Information : \\nTrade Name:  \\nClassification Name:  \\nCommon Name:  \\nRegulation:  \\nProd uct Code : \\nReview Panel : \\nClass:  FFR angio System\\nCoronary Vascul ar Physiologi c Simulation Software \\nDevice \\nDigital FFR System \\n21 CFR 870.1415 \\nQEK \\nCardiovascular \\nII \\nE. Predicate Device (s): K161772  for FFR CT, manufactured by HeartFlow, Inc.  \\nF. Summary Device Description:\\n[Quote from Document] assessment of coronary arteries.  The results of HeartFlow FFR\\nCT are intended to be used by qualified \\nclinicians in conjunction with the patients clinical  history, symptoms, and other diagnostic tests, as \\nwell as the clinicians professional judgment. \\n \\n5.5.1 Contraindications \\nThe FFR CT v2.0 Customer Instructions for Use ( VOL_003 Instructions for Use  Customers ) clearly \\nidentify for which patient populations and CT scanner manufacturers the product has been \\nclinically validated.  \\n5.5.2 Warnings and Precautions \\nThe warnings and precautions can be found in the FFR CT v2.0 product labeling ( VOL_003 \\nInstructions for Use  Customers ). \\n \\n5.6 Technological Characteristics of Device \\nThe HeartFlow FFR CT device is a software medical device  that allows for the quantitative and \\nqualitative analysis of Coronary Computed Tomography Angiography (cCTA).  FFR CT v2.0 is the next \\ngeneration of the predicate device FFR CT v1.4 and has the same technological characteristics.\\n[Quote from Document] provided  to support  qualified clinicians  \\nto aid  in the evaluation and \\nassessment of co ronary arteries. The \\nresults of HeartFlow FFR CT are \\nintended to be used by qualified \\nclinicians in conjunction with the \\npatients clinical history, symptoms, \\nand other diagnostic tests, as well as \\nthe clinicians professional  judgment.  \\nSystem \\noverview  Com puter system with software that \\nconstructs and displays a 3D \\ncomputer model of the coronary \\narteries to simulate blood flow  Computer system with software that \\nconstructs and displays a 3D computer \\nmodel of  the coronary arteries to \\nsimulate blood flow  \\nImag e source  Standard DICOM angiographic \\nimages taken in Cath Lab  Previously obtained CT images  \\nSoftware \\ncontrolled  Yes Yes \\nOn-line \\nprocessing  Yes No \\nSensitivity*  93.5% (lower 95% CI, 87.8% ) 84.2% (lower 95% CI, 75.8% ) \\nSpecificity*  91.2% (lower 95% CI, 86.0%) 84.9% (lower 95% CI, 80.4% )\\n[Quote from Document] 510(k) Number (if known)\\nK182149\\nDevice Name\\n \\nFFRangio\\nIndications for Use (Describe)\\n \\nCathWorks FFRangio is a software device for the clinical quantitative and qualitative analysis of previously acquired angiography DICOM data for patients with coronary artery disease. It provides FFRangio, a mathematically derived quantity, computed from simulated blood flow information obtained from a 3D computer model, generated from coronary angiography images. FFRangio analysis is intended to support the functional evaluation of coronary artery disease. The results of this analysis are provided as a supportive aid for qualified clinicians in the evaluation and assessment of coronary arteries physiology. The results of CathWorks FFRangio are intended to be used by qualified clinicians in conjunction with the patients clinical history, symptoms, and other diagnostic tests, as well as the clinicians professional evaluation. \\nType of Use (Select one or both, as applicable)\\n'}\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://2d22b00102d643d55c.gradio.live\n",
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
        "demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
        "\n",
        "try:\n",
        "    demo.launch(debug=True, share=True, show_api=False)\n",
        "    demo.close()\n",
        "except Exception as e:\n",
        "    demo.close()\n",
        "    print(e)\n",
        "    raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yCb3RVVfbmQ0",
      "metadata": {
        "id": "yCb3RVVfbmQ0"
      },
      "source": [
        "<br>\n",
        "\n",
        "----\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Part 4:** Saving Your Index For Evaluation\n",
        "\n",
        "After you've implemented your RAG chain, please save your accumulated vector store as shown [in the official documentation](https://python.langchain.com/docs/integrations/vectorstores/faiss#saving-and-loading). You'll have a chance to use it again for your final assessment!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y4se5wQ4Afda",
      "metadata": {
        "id": "Y4se5wQ4Afda"
      },
      "outputs": [],
      "source": [
        "## Save and compress your index\n",
        "docstore.save_local(\"docstore_index\")\n",
        "!tar czvf docstore_index.tgz docstore_index\n",
        "\n",
        "!rm -rf docstore_index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LsI7NivbIgFw",
      "metadata": {
        "id": "LsI7NivbIgFw"
      },
      "source": [
        "If everything was properly saved, the following line can be invoked to pull the index from the compressed `tgz` file (assuming the pip requirements are installed). After you have confirmed that the cell can pull in your index, download `docstore_index.tgz` for use in the last notebook!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qs8820ucIu1t",
      "metadata": {
        "id": "Qs8820ucIu1t"
      },
      "outputs": [],
      "source": [
        "## Assuming you haven't imported these yet\n",
        "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "embedder = NVIDIAEmbeddings(model=\"nvolveqa_40k\")\n",
        "!tar xzvf docstore_index.tgz\n",
        "new_db = FAISS.load_local(\"docstore_index\", embedder)\n",
        "docs = new_db.similarity_search(\"Testing the index\")\n",
        "print(docs[0].page_content[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "as_3vWJGKB2F",
      "metadata": {
        "id": "as_3vWJGKB2F"
      },
      "source": [
        "-----\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Part 5:** Wrap-Up\n",
        "\n",
        "Congratulations! Assuming your RAG chain is all good, you're now ready to move on to the **RAG Evaluation [Assessment]** section!\n",
        "\n",
        "### <font color=\"#76b900\">**Great Job!**</font>\n",
        "\n",
        "### **Next Steps:**\n",
        "1. **[Optional]** Revisit the **\"Questions To Think About\" Section** at the top of the notebook and think about some possible answers.\n",
        "2. Continue on to the next video, which will discuss **RAG Evaluation**.\n",
        "3. After the video, continue on to the corresponding notebook on **RAG Evaluation [Assessment]**.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8098de2f-32b3-428e-8f3b-f54141ec40b4",
      "metadata": {
        "id": "8098de2f-32b3-428e-8f3b-f54141ec40b4"
      },
      "source": [
        "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
